\documentclass{scrbook}
\usepackage{brian}
\usepackage{epigraph}

\usepackage[style=alphabetic,backend=bibtex]{biblatex}
\bibliography{truth}

\usepackage{graphicx}
\usepackage{colortbl}

\renewcommand{\implies}{\to}
\renewcommand{\iff}{\leftrightarrow}
\begin{document}
Mathematics deals with absolute truth, right? Errr, well, absolute truth is an incredibly loaded term. Properly unpacking it requires a healthy amount of philosophy and mathematics. Wait. Don't go away. It's fun. You'll be grappling with paradoxical statements like `this sentence is false' in a productive and well-defined way. You'll find statements that are neither true nor false, yet are critical to our modern understanding of mathematics. 

\chapter{Philosophical Underpinnings}
\section[Won't get Fooled Again]{won't get fooled again}
\epigraph{I'll tip my hat to the new constitution \\
Take a bow for the new revolution \\
Smile and grin at the change all around \\
Pick up my guitar and play \\
Just like yesterday \\
Then I'll get on my knees and pray \\
We don't get fooled again \\
Don't get fooled again \\
No, no!}{The Who}

Rhetoric is an ancient tradition of disguising opinions as sound logic. Through millions of years of evolution, we carbon sack computers have been tuned to take shortcuts. Taking shortcuts in thought is necessary: no one would teach an infant to count by formally proving $2+2=4$. But with these shortcuts come logical fallacies, passing off emotions as fact, perception as reality.

We could start with assertions and then work toward a conclusion by requiring each new claim we make must logically follow from what we have already established, but English is too ambiguous and irregular. Consider the sentence
\begin{quote}
  Every mouse fears some cat.
\end{quote}
Does this mean there is one terribly frightening cat that every single mouse fears? Or does it mean for each mouse there is a corresponding scary cat? \cite{wiki:cat-gen}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{images/tom-jerry}
  \caption{A scary cat }
\end{figure}

We need to move toward logical deductions governed by rules expressed in an appropriate notation (not English). These rules should be straight-forward enough that they can be checked by a computer, but expressive enough that we can work out all of mathematics.

We look to answer a nebulous philosophical question, `is mathematics absolutely true?' by building a model of logic and mathematics, and then asking our model what it can tell us about truth. Instead of saying, yes it is absolutely true, or no it is not, we will look for algorithms that will check our proofs. Such an approach discards much nuance of the philosophical question. This punting is necessary because the question `is mathematics absolutely true' is rife with uncertainty---what is mathematics? what is absolute truth? does absolute truth exist? how can a human judge truth if `to err is human'? Objections \emph{ad nauseum}.  

We'll need to think logically to reason about our model. This circular reasoning can't be helped. It does pose a philosophical problem---we may very well be in the matrix, in which case everything we know is a lie, so our mathematical model could be dead wrong. I will assume without proof that we are not in the matrix. I will assume without proof that logic-checking computers do not lie. I will assume without proof that this circular reasoning is innocent. 
\chapter[Boolean Logic]{boolean logic}
The simplest interesting model of logic we will discuss. There are two possible values, true and false. When it is not ambiguous, we will abbreviate these as $0$ and $1$. Letters are variables. There are three operators we start with: and ($\wedge$), or ($\vee$), and not ($\neg$). 
\begin{defn}[and $\wedge$]
  \label{def:bool:and}
  $x \wedge y$ is true exactly when $x$ and $y$ are both true. This is also called conjunction.
%BEGIN RECEIVE ORGTBL and-tt
  \begin{center}
    \begin{tabular}{r|rr}
      $\wedge$ & 0 & 1 \\
      \hline
      0 & 0 & 0 \\
      1 & 0 & 1 
    \end{tabular}
  \end{center}
%END RECEIVE ORGTBL and-tt
\end{defn}

\begin{defn}[or $\vee$]
  \label{def:bool:or}
  $x \vee y$ is true exactly when either $x$ or $y$ or both are true. This is also called disjunction.
%BEGIN RECEIVE ORGTBL or-tt
  \begin{center}
\begin{tabular}{r|rr}
$\vee$ & 0 & 1 \\
  \hline
0 & 0 & 1 \\
1 & 1 & 1 
\end{tabular}
\end{center}
%END RECEIVE ORGTBL or-tt
\end{defn}
And
\begin{defn}[not $\neg$]
Negation flips $0$ and $1$:
%BEGIN RECEIVE ORGTBL not-tt  
  \begin{center}
\begin{tabular}{rr}
$x$ & $\neg x$ \\
0 & 1 \\
1 & 0 
\end{tabular}
\end{center}
%END RECEIVE ORGTBL not-tt
\end{defn}

\begin{theorem}[De Morgan's law]
  \begin{align}
    \neg(x\wedge y) &=  (\neg x) \vee (\neg y) \label{eq:dmorg:and} \\
    \neg(x \vee y) &= (\neg x) \wedge (\neg y) \label{eq:dmorg:or}
  \end{align}
  (see also \cite{wiki:demorgan})
\end{theorem}
\begin{proof}
 First show \cref{eq:dmorg:and} by checking truth tables:
 \begin{center}
%BEGIN RECEIVE ORGTBL nand-dmorg
\begin{tabular}{rrrrrrr}
$x$ & $y$ & $x\wedge y$ & $\neg(x\wedge y)$ & $\neg x$ & $\neg y$ & $(\neg x)\vee (\neg y)$ \\
\hline
0 & 0 & 0 & 1 & 1 & 1 & 1 \\
0 & 1 & 0 & 1 & 1 & 0 & 1 \\
1 & 0 & 0 & 1 & 0 & 1 & 1 \\
1 & 1 & 1 & 0 & 0 & 0 & 0 
\end{tabular}
%END RECEIVE ORGTBL nand-dmorg
\end{center}
Then negate both sides of $\cref{eq:dmorg:and}$, giving 
\[
x\wedge y = \neg(\neg x \vee \neg y)
\]
Make the substitution $\bar x = \neg x$ and $\bar y =\neg y$:
\[
(\neg \bar x)\wedge (\neg \bar y) = \neg(\bar x \vee \neg \bar y)
\]
or, equivalently, \cref{eq:dmorg:or}. 
\end{proof}

\section[The Conditional]{the conditional}
We would like to model causality. Causality is vital in logical reasoning. But defining causality is a black hole ready to swallow up even the cleverest of thinkers. So hark the olde adage `the best mathematician is a lazy one', and punt. We will define the operator implies, a.k.a. the conditional, denoted $\implies$ to act as our stripped-down notion of causality. 
\begin{defn}[implies $\implies$]
  $x\implies y$ is true for all inputs except $1\implies 0$. 
\begin{center}  
%BEGIN RECEIVE ORGTBL if-tt
\begin{tabular}{r|rr}
$\implies$ & 0 & 1 \\
\hline
0 & 1 & 1 \\
1 & 0 & 1 \\
\end{tabular}
%END RECEIVE ORGTBL if-tt
\end{center}
\end{defn}
We like this definition because $x\implies y$ captures the idea $y$ must follow from $x$ being true, while allowing for the possibility that $y$ may occur even when $x$ is false. For example, if $x$ means `that's a cat', and $y$ means `that's a mammal', $x\implies y$ means `that's a cat, hence it's a mammal', with the unspoken caveat that dogs are not cats ($x=0$), but they are mammals ($y=1$). 

That example is awkward because we have no good way of saying `all cats are mammals, but not all mammals are cats' in boolean logic. We can say this, but not in a particularly useful way. In the next chapter, we will construct a more sophisticated logic that allows such statements. 

We defined the conditional through a truth table; however, our definition of boolean logic does not mention truth tables. We ought to give a definition of $\implies$ using $\wedge$, $\vee$, and $\neg$:
\begin{theorem}
  $x\implies y$ is equivalent to $y \vee (\neg x)$
\end{theorem}
\begin{proof}
  Both expressions are false only when $x=1$ and $y=0$. 
\end{proof}

\begin{defn}[logical equivalence $\iff$]
  $x\iff y$ exactly when $x = y$. 
\end{defn}
Likewise, we can define this in terms of $\wedge$, $\vee$, and $\neg$:
\begin{theorem}
  $x\iff y$ is equivalent to $(x \wedge y) \vee (\neg x \wedge \neg y)$. 
\end{theorem}
\section[Tables to Expressions]{tables to expressions}
We have just translated two truth-tables into expressions consisting only of $\wedge$, $\vee$, and $\neg$. You might wonder if we can always do this. We can. 
\begin{theorem}[completeness]
  Every truth table has a corresponding boolean expression.
\end{theorem}
\begin{proof}
  It's best to start with an example. Consider \cref{tt:complete}
\definecolor{LightCyan}{rgb}{0.8,1,1}
  \begin{table}[h!]
    \centering
  \caption{some truthtable}\label{tt:complete}
%BEGIN RECEIVE ORGTBL bool-complete
\begin{tabular}{rrrr}
$x_1$ & $x_2$ & $x_3$ & $f$ \\
\hline
0 & 0 & 0 & 0 \\
\rowcolor{LightCyan} 0 & 0 & 1 & 1 \\
0 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 \\
\rowcolor{LightCyan} 1 & 0 & 0 & 1 \\
1 & 0 & 1 & 0 \\
\rowcolor{LightCyan} 1 & 1 & 0 & 1 \\
1 & 1 & 1 & 0 
\end{tabular}
%END RECEIVE ORGTBL bool-complete
\end{table}
We want an expression that is true exactly when $(x_1,x_2,x_3)$ is $(0,0,1)$ or $(1,0,0)$ or $(1,1,0)$. A straightforward way to do this is to or together a statement that is true only on the input $(0,0,1)$, a statement true only on $(1,0,0)$, and a statement that is true only on $(1,1,0)$. 

Consider $(x_1,x_2,x_3)=(0,0,1)$. To find an expression that is true for this input and false for all others, we can simply assert $x_1=0$ and $x_2=0$ and $x_3=1$, \emph{i.e.}
\[
\neg x_1\wedge \neg x_2 \wedge x_3
\]
By the same argument, we can find the remaining expressions:
\[
\begin{matrix}
 (x_1,x_2,x_3)=(0,0,1) & \neg x_1 \wedge \neg x_2 \wedge x_3 \\
 (x_1,x_2,x_3)=(1,0,0) & x_1 \wedge \neg x_2 \wedge \neg x_3 \\
 (x_1,x_2,x_3)=(1,1,0) & x_1 \wedge x_2 \wedge \neg x_3 
\end{matrix}
\]
Hence \cref{tt:complete} can be written as 
\[
(\neg x_1 \wedge \neg x_2 \wedge x_3 ) \vee (x_1 \wedge \neg x_2 \wedge \neg x_3) \vee  (x_1 \wedge x_2 \wedge \neg x_3)
\]

To prove the theorem, we just need to generalize the work we've done. Suppose we are given a truth table for the function $f(x_1,\dots, x_n)$. Mark off the rows in the truth table where $f(x_1,\dots,x_n)=1$. For each such row, we construct the expression that asserts the input matches. If the current row is for the input
\[
(x_1,\dots, x_n)=(\alpha_1,\dots, \alpha_n)
\]
then the expression is
\[
\left (
  \begin{matrix}
    x_1 & \textrm{ if } \alpha_1=1 \\
    \neg x_1 & \textrm { if } \alpha_1=0
  \end{matrix}
\right)
\wedge
\dots
\wedge
\left (
  \begin{matrix}
    x_n & \textrm{ if } \alpha_n=1 \\
    \neg x_n & \textrm { if } \alpha_n=0
  \end{matrix}
\right)
\]
Abbreviate this as $(x_1=\alpha_1)\wedge \dots \wedge (x_n=\alpha_n)$. This is just a notational convenience; the final answer will not actually contain $=$ or refer to the $\alpha_i$. 

Then we or over each of the above expressions: 
\[
f(x_1,\dots,x_n)= \bigvee_{\alpha_1,\dots,\alpha_n: f(\alpha_1,\dots,\alpha_n)=1} (x_1=\alpha_1)\wedge \dots \wedge (x_n=\alpha_n)
\]
giving a boolean expression with the desired truth table. 
\end{proof}
This tells us that boolean logic is big enough to express any truth table. An interesting implication is that general purpose computers can be constructed!
\section[Absolute Truth?]{absolute truth?}
We are supposed to shy away from notions of absolute truth, but boolean logic is simple enough that we can talk about it without making a mess. A boolean expression's truth value can be computed directly from the inputs. Since each input can be either true or false, there are only finitely many possible inputs. Hence, everything you need to know is contained in the expression's truth table. These computations can be carried out unambiguously by a machine in finite time, so things are looking pretty good.

We can also show there are no contradictions. In boolean logic, a contradiction would be of the form
\[
p \wedge \neg p
\]
for some expression $p$. No matter what the expression $p$ is, it must either evaluate to $1$ or $0$. Suppose $p=1$. Then $\neg p=0$. So $p\wedge \neg p = 1 \wedge 0 =0$. Likewise, if $p=0$, we get $0\wedge 1=0$. Thus all contradictions are false.

\chapter[Predicate Logic]{predicate logic}
Boolean logic is beautiful, but it is not expressive enough to make deductions like `Socrates is a man. All men are mortal. Therefore, Socrates is mortal.' Enter Gottlob Frege. 

Nineteenth century mathematics was in crisis. For thousands of years, mathematics had remained essentially unchanged: the gold standard was proof by Euclidean geometry. Algebra, arabic numerals, zero were tools that expedite geometric reasoning. Even new results in number theory were assumed to be, at heart geometric truths. But by the 1800s, it was clear this worldview was no longer sustainable. Mathematics had grown too abstract.

Calculus, the most revolutionary discovery this side of the dark ages, stood with embarrassingly poor theoretical footing. Newton and Leibniz through Euler and Cauchy relied on infinitesimals, but could not justify their use \emph{a priori}, certainly not by Euclidean geometry. To make matters worse, if you were not careful, an argument that seemed kosher could result in nonsense. 
Fourier's work was downright scandalous. By decomposing functions as an infinite series of $\sin$ and $\cos$ waves, he was able to solve the heat equation, a problem unsolved for all but the most trivial cases. Experiments verified his results, but conservative mathematicians could not come to terms with it. 

Abel, and later Galois, proved that degree $5$ equations have no general solution that can be written in terms of the basic arithmetic operations and radicals. In particular, the crux Galois's proof were abstract algebraic systems that were not clearly based in Euclidean geometry. 

Even Euclidean geometry was proving to be less than the paragon of flawless logic it once seemed. Problems in Euclidean geometry, such as is there a method to trisect an angle using a compass and straight edge, went unsolved for a few millennia. But these were quickly solved using Galois's tools. Euclid's parallel axiom was thought to be provable from his other axioms for thousands of years, but it was shown in the mid 19th century, that it was in fact independent, as Euclid's remaining axioms are satisfied by spherical and hyperbolic geometry, in both of which Euclid's parallel postulate does not hold. 

In the nineteenth century, mathematics outgrew its ontology. The situation was so disorienting, Dodgson, an Oxford mathematician better known as Lewis Carol, wrote \emph{Alice in Wonderland}, a veiled satire ridiculing the absurdity of this new mathematics. \cite{alice}

Gottlob Frege laid the foundations for mathematics' new ontology. Whereas George Boole's logic was viewed more as a useful trick that cleaned up some ambiguities for the pedants, Frege developed a logic capable of housing all of the new mathematical objects. His work went relatively unnoticed by his contemporaries, but it revolutionized mathematics and philosophy. 

\section[What is Predicate Logic?]{what is predicate logic?}
First order predicate logic is the modern descendant of Frege's logic. It is more expressive than boolean logic, making it both critical to our modern understanding of mathematics, and complicated enough that mathematicians have given up deciding if it is absolutely true. 

Statements in predicate logic are made up of the logical symbols:
\begin{trivlist}
\item quantifiers: for every ($\forall$) and there exists ($\exists$)
\item logical connectives: $\wedge,\vee,\neg,\implies,\iff$
\item parenthesis 
\item variables: lowercase letters
\item equality: $=$
\end{trivlist}

and additionally 
\begin{trivlist}
\item predicate symbols: capital letters $P(x,y,z)$ or typewriter text $\texttt{even?(n)}$.
\item function symbols 
\item constant symbols  
\end{trivlist}
The new entities are the quantifiers, the predicate symbols, the function symbols, and the constants. Function symbols and constant symbols probably do what you expect: function symbols denote functions, and constant symbols denote constants. 

Predicates denote true-or-false propositions of their arguments. For example $\geq$ is a predicate symbol that is understood to mean `greater than or equal to'. 

Finally, quantifiers allow us to form statements like `every man is mortal'. If $\texttt{man?}$ and $\texttt{mortal?}$ are the predicates for, well, `is the argument a man?' and `is the argument mortal?', this sentence translates to
\[
\forall x ( \texttt{man?}(x) \implies \texttt{mortal?}(x) )
\]
which says roughly, for all $x$ in the universe, if $x$ is a man, then $x$ is mortal. 

If we refer to Socrates by the constant symbol $s$, we can state the syllogism `Socrates is a man. Men are mortal. Hence, Socrates is mortal' as follows:
\[
\texttt{man?}(s) \wedge \forall x \bigl(\texttt{man?}(x)\implies\texttt{mortal?}(x)\bigr) \implies \texttt{mortal?}(s)
\]

Recall the statement
\begin{quote}
  Every mouse fears some cat
\end{quote}
We can now state this unambiguously as 
\[
\forall m \bigr(\texttt{mouse?}(m) \implies \exists c (\texttt{cat?}(c)\wedge \texttt{fears?}(m,c)) \bigl)
\]
\emph{i.e.} `for every mouse there is a corresponding scary cat', or 
\[
\exists c \bigl(\texttt{cat?}(c) \forall m (\texttt{mouse?}(m) \texttt{fears?}(m,c)\bigr)
\]
\emph{i.e.} `there is one particularly frightening cat all mice fear'.

\section[Peano Arithmetic]{peano arithmetic}
I promised this would be sufficient to house modern mathematics. I will show you how the natural numbers can be defined using Peano's axioms \cite{wiki:peano}. 

\renewcommand{\succ}{\texttt{succ}}
First, stipulate there is a constant called $0$ and a function of one argument called $\succ$. 
The Peano axioms are as follows: 
\begin{trivlist}
\item Nothing comes before zero:
  \[
  \forall x. 0 \neq \succ(x)
  \]
\item Two naturals are equal if their successors are equal:
  \[
  \forall x,y. \succ (x) = \succ(y) \implies x=y
  \]
\item Induction: if $\Phi$ is a predicate, 
  \[
  \Phi(0) \wedge \forall x \bigl(\Phi(x)\implies \Phi(\succ(x))\bigr) \implies \forall y.\Phi(y)
  \]
  In English, if $\Phi(0)$ and $\Phi(s)$ implies $\Phi(s+1)$, then $\Phi$ must be true for all natural numbers. 
\end{trivlist}

\begin{defn}[plus]
  \label{defn:peano:+}
  We can define $+$, a function of two inputs, by recursion:
  \begin{align*}
    \forall x (x + 0 &= x) \\
    \forall x \forall y \bigl(x + \succ(y) = succ(x+y)\bigr)
  \end{align*}
\end{defn}
\section[Deduction]{deduction}
In boolean algebra, when determining the truth of a statement, the worst case scenario will require writing out a large truth table. Not so for predicate logic. Quantifiers range over everything in some possibly infinite universe. Predicates abstract away calculations to something behind the scenes. Without any way to explicitly enumerate everything in the universe, the logician must indicate its content, as well as the behavior of some predicates, with axioms.  

Some axioms, like $\forall x P(x)$ and $\exists x \neg P(x)$ are contradictory, hence nonsensical. One would hope this is just a case of `ask a bad question; get a bad answer'. We would hope that the laws of deduction, which govern what you can actually do with the axioms you choose, are ok. 

Prior to stating the laws of deduction, we must define the following: 
\begin{defn}[bound variable]
  A variable is said to be bound if it is introduced by a quantifier. 
\end{defn}
\begin{example}[bound variables]
  In the expression
  \[
  \forall x (\texttt{even?}(x)\implies \exists y . x=2y)
  \]
  the variables $x$ and $y$ are bound.  
\end{example}
\begin{defn}[free variable]
  A variable that is not bound is called free. 
\end{defn}
\begin{example}[free variable]
  In 
  \[
  \forall y . x\leq 2y
  \]
  $y$ is bound but $x$ is free. 
\end{example}
\begin{defn}[scope]
  The scope of a bound variable is the part of the expression where the variable is understood to mean the variable introduced by a quantifier. We can make scope explicit with parenthesis:
  \[
  \forall x \bigl(\underbrace{\texttt{cat?}(x)\implies \texttt{mammal?}(x)}_{\textrm{scope}}\bigr)
  \]
  When no parenthesis are present, the scope is understood to lie between the quantifier and the first $\implies$ or $\iff$:
  \[
  \forall x . \underbrace{x\neq 0\wedge x \leq 3}_{\textrm{scope of $x$}}  \implies \neg \exists y . \underbrace{y > 3}_{\textrm{scope of $y$}}
  \]
\end{defn}

For $\wedge,\vee,\neg,\implies,\iff$, deduction works the same as it did for boolean logic:
\begin{trivlist}
\item $x \wedge y$ is true if and only if (iff) $x$ is true and $y$ is true
\item $x \vee y$ is true iff $x$ is true or $y$ is true 
\item $\neg x$ is true iff $x$ is false   
\item $x \implies y$ is true unless $x=1$ and $y=0$  
\item $x\iff y$ is true iff $x$ has the same truth value as $y$  
\end{trivlist}

For the additional deduction rules for the quantifiers,  we assume $\phi(x)$ is a predicate, possibly one that can be expressed as a first order statement. If $\phi(x)$ contains a bound instance of $x$, the bound $x$ will `shadow' the parameter. For example, 
\[
\phi(x)=\texttt{even?}(x) \wedge \forall x (\texttt{even?}(x)\vee \texttt{odd?}(x))
\]
would be more clearly defined as 
\[
\phi(y)=\texttt{even?}(x) \wedge \forall y (\texttt{even?}(y)\vee \texttt{odd?}(x))
\]
We will try to avoid statements like this to prevent a headache, but for accuracy's sake we note them here. 
\begin{trivlist}
\item Universal Introduction:
  If we know $\phi(\alpha)$, we can conclude $\forall \alpha . \phi(\alpha)$ so long as $\alpha$ is unused elsewhere.
\item Universal Elimination:
  If we know $\forall \alpha . \phi(\alpha)$ then we can conclude $\phi(\beta)$. 
\item Existential Introduction:
  If we know $\phi(\beta)$ then we can conclude $\exists \alpha \phi(\alpha)$ so long as $\alpha$ is previously unused.
\item Existential elimination:
  If we know $\exists \alpha \phi(\alpha)$, we can say $\phi(\beta)$ so long as $\beta$ is previously unused. 
\end{trivlist}
Applications of some rules may require renaming variables. This is no problem as long as the meaning of the statement is not changed (\emph{i.e.} two things that used to have different names cannot have the same name). 

\section[Some Proofs]{some proofs}
We are now prepared to prove things about peano arithmetic:

\begin{theorem}[$+$ is associative]
  The statement 
  \[
  \forall x \forall y \forall z . (x+y)+z = x+(y+z)
  \]
  is deducible from peano arithmetic and the definition of \nameref{defn:peano:+}.
\end{theorem}
\begin{proof}
  Short version---this is a proof by induction. First, we prove the base case:
   \begin{equation}
    \label{eq:peano:+assoc:base}
    \forall a \forall b .  (a+b) + 0 = a+(b+0)
  \end{equation}
  Equivalently, $\phi(0)$ where
  \[
  \phi(x) = \forall a \forall b . (a+b)+x = a+(b+x)
  \]
  Noting that $y+0=y$, (see the definition of  \nameref{defn:peano:+}), we 
  see \cref{eq:peano:+assoc:base} is, in fact, true. 
 
  Now we prove the inductive step: 
  \begin{equation}
    \label{eq:peano:+assoc:ind}
    \forall x \bigl(\phi(x)\implies \phi(\succ(x))\bigr)   
  \end{equation}
  Pick $a$, $b$, and $c$. 
  Assume $(a+b)+c = a+(b+c)$, which we abbreviate as $a+b+c$. Now consider $(a+b)+\succ(c)$. By definition of \nameref{defn:peano:+}, we see \[(a+b)+\succ(c)=\succ(a+b+c)\] Similarly, \[a+(b+\succ(c))=a+\succ(b+c)=\succ(a+b+c)\]
  hence $(a+b)+\succ(c)=a+(b+\succ(c))$. This proves the inductive step, \cref{eq:peano:+assoc:ind}.
  
  To finish the proof, use the induction axiom. 
\end{proof}

The above proof does not justify each step, but gives enough information to show how deduction works. This is the norm for mathematical proofs. Actually, such a proof is boring when you are studying the mathematics of the natural numbers: it is not hard to convince someone addition is commutative. 

But since we want to understand the very fabric of reason itself, it must be possible to write out this proof in such painstaking detail that a computer, or even the most pedantic of logic professors, can follow it. Such a proof may look like the following: 

\begin{proof}
  Full gory details---First establish the base case, \cref{eq:peano:+assoc:base}
  Consider the statement $(a+b)+0$. By \cref{defn:peano:+}, we know $\forall x . x+0=x$. By universal elimination, deduce $(a+b)+0=a+b$. Now consider
  $a+(b+0)$. By universal elimination again, deduce $b+0=b$. By substitution, deduce $a+(b+0)=a+b$. Then, by transitivity, we know $(a+b)+0=a+(b+0)$. As $a$ and $b$ are totally arbitrary free variables, two universal introductions establish \cref{eq:peano:+assoc:base}.

  Now the inductive step, \cref{eq:peano:+assoc:ind}
  Assume \begin{equation}(a+b)+c=a+(b+c)=a+b+c\label{eq:peano:+assoc:ass}\end{equation}
  Consider the statement $(a+b)+\succ (c)$. From \cref{defn:peano:+}, we know
  \[\forall x \forall y . x+\succ(y)=\succ(x+y)\]
  An analogous quantifier elimination gives $(a+b)+\succ (c) = \succ(a+b+c)$. 
  Another quantifier elimination gives $a+(b+\succ(c))= a+\succ(b+c)$. A final quantifier elimination gives $a+\succ(b+c)=\succ(a+b+c)$. Transitivity gives $(a+b)+\succ(c)=a+(b+\succ(c))$. We can stop assuming \cref{eq:peano:+assoc:ass}, rephrasing our work so far as
  \[
  (a+b)+c=a+(b+c)\implies (a+b)+\succ(c) = a+(b+\succ (c))
  \]
  
  As we have shown \cref{eq:peano:+assoc:base,eq:peano:+assoc:ind} we can deduce 
  \begin{equation}
    \label{eq:peano:+assoc:both}
    \phi(0)\wedge \forall x \bigl(\phi(x)\implies \phi(\succ(x))\bigr)
  \end{equation}
  By the induction axiom, 
  \[
  \phi(0)\wedge \forall x \bigl(\phi(x)\implies \phi(\succ(x))\bigr) \implies \forall x \phi(x)
  \]
  Then using boolean logic (\emph{modus ponens}), conclude $\forall z \phi(z)$. Hence, $+$ is associative. This proves the theorem. 
\end{proof}

Ironically, justifying each step results in a proof that is harder for a human to understand. 
\chapter[The Death of Reason]{the death of reason}
Once mathematicians started actually reading Frege, it was not long until they would dream the impossible dream. Hilbert and his entourage longed for a world in which mathematics had been effectively solved, the right axioms were chosen, and a machine would be built that would write out every true theorem until the end of time. All mathematicians would then be free to sip martinis poolside as an endless tickertape listing all true things would scroll by. \cite{wiki:hilbert-prog}

The mathematicians would have to really read this carefully every now and then to answer a physicist's question. But they discovered by speaking in gratuitous generalities and shouting `the proof is trivial' rather frequently, they could make a pesky physicist disappear, ensuring a veritable utopia (see \url{http://www.smbc-comics.com/index.php?db=comics&id=2675#comic}). Thus French Formalism was born.  

But, one fateful night in 1931, dear reader, a young Kurt Gödel dealt a fatal blow to the Hilbertian dream of mathematical rapture. He was merely engaging in the logician's favorite pass-time: making things mercilessly meta\footnote{I'm so cool}. As many a logician and patron of webcomics know, making things meta may make a mess, and a major mess it made moreover. To his merit, this metamethematical misfortune marks the makings of a modern mathematical maturity, one more meaningful than a mere mirage marred by the multitude of mathematical models and their unmarried meandering truths. Any system of mathematics sufficiently sophisticated to serve as semantics for something like peano's arithmetic suffers from a malady said to be incompleteness. See, m'lady, such a malady means more than several true-seeming statements may lay beyond reach of the most surefire machines of Hilbert's dreams.

\printbibliography
\end{document}
#+ORGTBL: SEND and-tt orgtbl-to-latex :splice nil :skip 0 
| $\wedge$ | 0 | 1 |
|        0 | 0 | 0 |
|        1 | 0 | 1 |
#+TBLFM: @2$2..@3$3=@1 && $1

#+ORGTBL: SEND or-tt orgtbl-to-latex :splice nil :skip 0 
| $\vee$ | 0 | 1 |
|      0 | 0 | 1 |
|      1 | 1 | 1 |
#+TBLFM: @2$2..@3$3=$1 || @1

#+ORGTBL: SEND not-tt orgtbl-to-latex :splice nil :skip 0 
| $x$ | $\neg x$ |
|   0 |        1 |
|   1 |        0 |

#+ORGTBL: SEND nand-dmorg orgtbl-to-latex :splice nil :skip 0 
| $x$ | $y$ | $x\wedge y$ | $\neg(x\wedge y)$ | $\neg x$ | $\neg y$ | $(\neg x)\vee (\neg y)$ |
|-----+-----+-------------+-------------------+----------+----------+-------------------------|
|   0 |   0 |           0 |                 1 |        1 |        1 |                       1 |
|   0 |   1 |           0 |                 1 |        1 |        0 |                       1 |
|   1 |   0 |           0 |                 1 |        0 |        1 |                       1 |
|   1 |   1 |           1 |                 0 |        0 |        0 |                       0 |
#+TBLFM: $3=$1 && $2::$4=! $3::$5=!$1::$6=!$2::$7=$5 || $6

#+ORGTBL: SEND if-tt orgtbl-to-latex :splice nil :skip 0 
| $\implies$ | 0 | 1 |
|------------+---+---|
|          0 | 1 | 1 |
|          1 | 0 | 1 |

#+ORGTBL: SEND bool-complete orgtbl-to-latex :splice nil :skip 0
| $x_1$ | $x_2$ | $x_3$ | f |
|-------+-------+-------+---|
|     0 |     0 |     0 | 0 |
|     0 |     0 |     1 | 1 |
|     0 |     1 |     0 | 0 |
|     0 |     1 |     1 | 0 |
|     1 |     0 |     0 | 1 |
|     1 |     0 |     1 | 0 |
|     1 |     1 |     0 | 1 |
|     1 |     1 |     1 | 0 |


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
