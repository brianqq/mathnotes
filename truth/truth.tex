\documentclass{scrbook}
\usepackage{brian}
\usepackage{epigraph}

\usepackage[style=alphabetic,backend=bibtex]{biblatex}
\bibliography{truth}

\usepackage{graphicx}
\usepackage{colortbl}
\usepackage[inline]{asymptote}

\renewcommand{\implies}{\to}
\renewcommand{\iff}{\leftrightarrow}

\newcommand{\Q}{\mathbb{Q}}

\usepackage{xspace}
\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\eg}{\emph{e.g.}\xspace}
\newcommand{\ord}{\mathtt{Ord}}

\usepackage{tocstyle}
\usetocstyle{nopagecolumn}
\newcommand*{\oldaddchaptertocentry}{}
\let\oldaddchaptertocentry\addchaptertocentry
\renewcommand*{\addchaptertocentry}[2]{%
  \ifstr{#1}{}{%
    \oldaddchaptertocentry{#1}{#2}%
  }{% entry with number
    \oldaddchaptertocentry{\color{gray}\fontsize{72pt}{0pt}\bfseries{{#1}}}{\texorpdfstring{\kern-3em{#2\vspace{10pt}}}{#2}}}%
  }
\settocstylefeature[0]{pagenumberhook}{\Huge}
\settocstylefeature[1]{entryhook}{\kern-3em}



\begin{document}
\tableofcontents{}
\chapter[Philosophical Underpinnings]{philosophical underpinnings}
Rhetoric could be described as the art of passing off opinions as objective. Through millions of years of evolution, we carbon sack computers have been tuned to take shortcuts. Even in the study of mathematics, these shortcuts are necessary: no one would teach an infant to count by formally proving $2+2=4$. But with these shortcuts leave us susceptible to logical fallacies, allowing rhetoric to pas off emotions as fact and perceptions as reality.

We could start with assertions and then work toward a conclusion by requiring each new claim we make must logically follow from what we have already established, but English is too ambiguous and irregular. Consider the sentence
\begin{quote}
  Every mouse fears some cat.
\end{quote}
Does this mean there is one terribly frightening cat that every single mouse fears? Or does it mean for each mouse there is a corresponding scary cat? \cite{wiki:cat-gen}
\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth]{images/tom-jerry}
  \caption{A scary cat }
\end{figure}

The solution? A deductive system. One with a well-defined language for clear expression and well-defined rules for clear argumentation. It should be straight-forward enough that proofs can be checked by a computer, so we can claim absolute, or at least unambiguous, truth. To be useful, a deductive system must be expressive enough that we can work out all of mathematics. There is no right balance of these two concerns; each deductive systems balance them differently. They are not at odds with each other, although they seem to describe distinctive worlds. 

I will be dodging the question `is mathematics absolutely true?' In my eyes, a satisfying answer to this question must come from a mathematical analysis. This question is rather resilient to mathematical analysis, thanks to the ambiguity of natural language, the philosophical pitfalls absolutes bring, and even limitations on the mathematical end. Instead, we will pose a series of mathematical questions that approximate `is mathematics absolutely true?', particularly, `can this argument be algorithmically checked?' and `are the valid arguments sensible?'

If we can algorithmically check arguments, we can approach a system of thought immune to subjectivity. This is not to say it is absolutely correct: if we're wrong, then we'll all be wrong together. We will elaborate on what sensible means as we go, but a `sensible' argument should be free from contradictions, and should either agree with our intuition, or be able to show us the error in our ways. 

How is this done? 
We will build a model of logical reasoning. With this scaffolding, we will build a model of mathematics. By studying these models, we glean some information about truth in mathematics. 


We'll need logic to reason about our model. This circular reasoning can't be helped. It does pose a philosophical problem---we may very well be in the matrix, in which case everything we know is a lie, so our mathematical model could be dead wrong. I will assume without proof that we are not in the matrix. I will assume without proof that logic-checking computers do not lie. I will assume without proof that this circular reasoning is innocent. 
\chapter[Boolean Logic]{boolean logic}
The simplest interesting model of logic we will discuss. There are two possible values, true and false. When it is not ambiguous, we will abbreviate these as $0$ and $1$. Letters are variables. There are three operators we start with: and ($\wedge$), or ($\vee$), and not ($\neg$). 
\begin{defn}[and $\wedge$]
  \label{def:bool:and}
  $x \wedge y$ is true exactly when $x$ and $y$ are both true. This is also called conjunction.
%BEGIN RECEIVE ORGTBL and-tt
  \begin{center}
    \begin{tabular}{r|rr}
      $\wedge$ & 0 & 1 \\
      \hline
      0 & 0 & 0 \\
      1 & 0 & 1 
    \end{tabular}
  \end{center}
%END RECEIVE ORGTBL and-tt
\end{defn}

\begin{defn}[or $\vee$]
  \label{def:bool:or}
  $x \vee y$ is true exactly when either $x$ or $y$ or both are true. This is also called disjunction.
%BEGIN RECEIVE ORGTBL or-tt
  \begin{center}
\begin{tabular}{r|rr}
$\vee$ & 0 & 1 \\
  \hline
0 & 0 & 1 \\
1 & 1 & 1 
\end{tabular}
\end{center}
%END RECEIVE ORGTBL or-tt
\end{defn}
\begin{defn}[not $\neg$]
Negation flips $0$ and $1$:
%BEGIN RECEIVE ORGTBL not-tt  
  \begin{center}
\begin{tabular}{rr}
$x$ & $\neg x$ \\
0 & 1 \\
1 & 0 
\end{tabular}
\end{center}
%END RECEIVE ORGTBL not-tt
\end{defn}

\begin{theorem}[De Morgan's law]
  \begin{align}
    \neg(x\wedge y) &=  (\neg x) \vee (\neg y) \label{eq:dmorg:and} \\
    \neg(x \vee y) &= (\neg x) \wedge (\neg y) \label{eq:dmorg:or}
  \end{align}
  (see also \cite{wiki:demorgan})
\end{theorem}
\begin{proof}
 First show \cref{eq:dmorg:and} by checking truth tables:
 \begin{center}
%BEGIN RECEIVE ORGTBL nand-dmorg
\begin{tabular}{rrrrrrr}
$x$ & $y$ & $x\wedge y$ & $\neg(x\wedge y)$ & $\neg x$ & $\neg y$ & $(\neg x)\vee (\neg y)$ \\
\hline
0 & 0 & 0 & 1 & 1 & 1 & 1 \\
0 & 1 & 0 & 1 & 1 & 0 & 1 \\
1 & 0 & 0 & 1 & 0 & 1 & 1 \\
1 & 1 & 1 & 0 & 0 & 0 & 0 
\end{tabular}
%END RECEIVE ORGTBL nand-dmorg
\end{center}
Then negate both sides of \cref{eq:dmorg:and}, giving 
\[
x\wedge y = \neg(\neg x \vee \neg y)
\]
Make the substitution $\bar x = \neg x$ and $\bar y =\neg y$:
\[
(\neg \bar x)\wedge (\neg \bar y) = \neg(\bar x \vee \neg \bar y)
\]
or, equivalently, \cref{eq:dmorg:or}. 
\end{proof}

\section[The Conditional]{the conditional}
We would like to model causality. Causality is vital in logical reasoning. But defining causality is a black hole ready to swallow up even the cleverest of thinkers.  Instead we'll define the operator implies, a.k.a. the conditional, denoted $\implies$, a stripped-down notion of causality. 
\begin{defn}[implies $\implies$]
  $x\implies y$ is true for all inputs except $1\implies 0$. 
\begin{center}  
%BEGIN RECEIVE ORGTBL if-tt
\begin{tabular}{r|rr}
$\implies$ & 0 & 1 \\
\hline
0 & 1 & 1 \\
1 & 0 & 1 \\
\end{tabular}
%END RECEIVE ORGTBL if-tt
\end{center}
\end{defn}
This definition of captures the idea $y$ must follow if $x$ is true. But it allows for the possibility that $y$ may occur even when $x$ is false. For example, if $x$ means `that's a cat', and $y$ means `that's a mammal', $x\implies y$ means `that's a cat, hence it's a mammal', with the unspoken caveat that dogs are not cats ($x=0$), but they are mammals ($y=1$). 

We defined the conditional through a truth table; however, our definition of boolean logic does not mention truth tables. We ought to give a definition of $\implies$ using $\wedge$, $\vee$, and $\neg$:
\begin{theorem}
  $x\implies y$ is equivalent to $y \vee (\neg x)$
\end{theorem}
\begin{proof}
  Both expressions are false only when $x=1$ and $y=0$. 
\end{proof}

\begin{defn}[logical equivalence $\iff$]
  $x\iff y$ exactly when $x = y$. 
\end{defn}
Likewise, we can define this in terms of $\wedge$, $\vee$, and $\neg$:
\begin{theorem}
  $x\iff y$ is equivalent to $(x \wedge y) \vee (\neg x \wedge \neg y)$. 
\end{theorem}
\section[Tables to Expressions]{tables to expressions}
We have just translated two truth-tables into expressions consisting only of $\wedge$, $\vee$, and $\neg$. You might wonder if we can always do this. We can. 
\begin{theorem}[completeness]
  Every truth table has a corresponding boolean expression.
\end{theorem}
\begin{proof}
  It's best to start with an example. Consider \cref{tt:complete}
\definecolor{LightCyan}{rgb}{0.8,1,1}
  \begin{table}[h!]
    \centering
  \caption{some truthtable}\label{tt:complete}
%BEGIN RECEIVE ORGTBL bool-complete
\begin{tabular}{rrrr}
$x_1$ & $x_2$ & $x_3$ & $f$ \\
\hline
0 & 0 & 0 & 0 \\
\rowcolor{LightCyan} 0 & 0 & 1 & 1 \\
0 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 \\
\rowcolor{LightCyan} 1 & 0 & 0 & 1 \\
1 & 0 & 1 & 0 \\
\rowcolor{LightCyan} 1 & 1 & 0 & 1 \\
1 & 1 & 1 & 0 
\end{tabular}
%END RECEIVE ORGTBL bool-complete
\end{table}
We want an expression that is true exactly when $(x_1,x_2,x_3)$ is $(0,0,1)$ or $(1,0,0)$ or $(1,1,0)$. A straightforward way to do this is to or together a statement that is true only on the input $(0,0,1)$, a statement true only on $(1,0,0)$, and a statement that is true only on $(1,1,0)$. 

Consider $(x_1,x_2,x_3)=(0,0,1)$. To find an expression that is true for this input and false for all others, we can simply assert $x_1=0$ and $x_2=0$ and $x_3=1$, \ie
\[
\neg x_1\wedge \neg x_2 \wedge x_3
\]
By the same argument, we can find the remaining expressions:
\[
\begin{matrix}
 (x_1,x_2,x_3)=(0,0,1) & \neg x_1 \wedge \neg x_2 \wedge x_3 \\
 (x_1,x_2,x_3)=(1,0,0) & x_1 \wedge \neg x_2 \wedge \neg x_3 \\
 (x_1,x_2,x_3)=(1,1,0) & x_1 \wedge x_2 \wedge \neg x_3 
\end{matrix}
\]
Hence \cref{tt:complete} has expression 
\[
(\neg x_1 \wedge \neg x_2 \wedge x_3 ) \vee (x_1 \wedge \neg x_2 \wedge \neg x_3) \vee  (x_1 \wedge x_2 \wedge \neg x_3)
\]

To prove the theorem, we just need to generalize the work we've done. Suppose we are given a truth table for the function $f(x_1,\dots, x_n)$. Mark off the rows in the truth table where $f(x_1,\dots,x_n)=1$. For each such row, we construct the expression that asserts the input matches. If the current row is for the input
\[
(x_1,\dots, x_n)=(\alpha_1,\dots, \alpha_n)
\]
then the expression is
\[
\left (
  \begin{matrix}
    x_1 & \textrm{ if } \alpha_1=1 \\
    \neg x_1 & \textrm { if } \alpha_1=0
  \end{matrix}
\right)
\wedge
\dots
\wedge
\left (
  \begin{matrix}
    x_n & \textrm{ if } \alpha_n=1 \\
    \neg x_n & \textrm { if } \alpha_n=0
  \end{matrix}
\right)
\]
Abbreviate this as $(x_1=\alpha_1)\wedge \dots \wedge (x_n=\alpha_n)$. This is just a notational convenience; the final answer will not actually contain $=$ or refer to the $\alpha_i$. 

Or over each of the above expressions: 
\[
f(x_1,\dots,x_n)= \bigvee_{\alpha_1,\dots,\alpha_n: f(\alpha_1,\dots,\alpha_n)=1} (x_1=\alpha_1)\wedge \dots \wedge (x_n=\alpha_n)
\]
giving the desired expression. 
\end{proof}
This tells us that boolean logic is big enough to express any truth table. An interesting implication is that general purpose computers can be constructed!
\section[Absolute Truth?]{absolute truth?}
Boolean logic is simple enough that we can talk about absolute truth without making a mess. Trusting boolean algebra is about as reasonable as trusting a calculator for integer arithmetic. 
A boolean expression's truth value can be computed directly from the inputs. Since each input can be either true or false, there are only finitely many possible inputs. Everything you need to know is contained in the expression's truth table. These computations can be carried out unambiguously by a machine in finite time, so things are looking pretty good.

A statement and its negation cannot be true simultaneously, \ie
\[
p \wedge \neg p = 0
\]
for any expression $p$. No matter what $p$ is, it must evaluate to either $1$ or $0$. Suppose $p=1$. Then $\neg p=0$. So $p\wedge \neg p = 1 \wedge 0 =0$. Likewise, if $p=0$, we get $0\wedge 1=0$. 

\chapter[Predicate Logic]{predicate logic}
Boolean logic is beautiful, but it is not expressive enough to make deductions like `Socrates is a man. All men are mortal. Therefore, Socrates is mortal.' 

Nineteenth century mathematics was in crisis. For thousands of years, mathematics had remained essentially unchanged: the gold standard was proof by Euclidean geometry. Algebra, arabic numerals, zero were tools to expedite geometric reasoning. Even new results in number theory were assumed to be, at heart, geometric truths. But by the 1800s, it was clear this worldview was no longer sustainable. Mathematics had grown too abstract.

Calculus, the most revolutionary discovery this side of the dark ages, stood with embarrassingly poor theoretical footing. Newton and Leibniz through Euler and Cauchy relied on infinitesimals, but could not justify their use \emph{a priori}, certainly not by Euclidean geometry. To make matters worse, if you were not careful, an argument that seemed kosher could result in nonsense. 
Fourier's work was downright scandalous. He was able to solve the heat equation, a problem unsolved for all but the most trivial cases. Experiments verified his results, but conservative mathematicians refused to accept his method.

Abel, and later Galois, proved that degree 5 equations have no general solution that can be written in terms of the basic arithmetic operations and radicals. In particular, the crux of Galois's proof were abstract systems with no clear base in Euclidean geometry. 

Even Euclidean geometry was proving to be less than the paragon of flawless logic it once seemed. Problems in Euclidean geometry went unsolved for millenia, like can you trisect an angle with a compass and straightedge went unsolved for a few millennia. But these were quickly solved using Galois's tools\cite{wiki:trisect}. Euclid's parallel axiom was thought to be provable from his other axioms for thousands of years, but it was shown in the mid 19th century that was impossible. Euclid's remaining axioms are satisfied by spherical and hyperbolic geometry. In each, Euclid's parallel postulate does not hold. 

In the nineteenth century, mathematics outgrew its ontology. The situation was so disorienting, Dodgson wrote a veiled satire ridiculing the absurdity of this new mathematics. Dodgson is better known as Lewis Carol. This satire is called \emph{Alice in Wonderland}.\cite{alice}

Gottlob Frege laid the foundations for mathematics' new ontology. Whereas George Boole's logic was viewed more as a useful trick that cleaned up some ambiguities for the pedants, Frege developed a logic capable of housing all of the new mathematical objects. His work went relatively unnoticed by his contemporaries, but it revolutionized mathematics and philosophy. 

\section[What is Predicate Logic?]{what is predicate logic?}
First order predicate logic is the modern descendant of Frege's logic. It is more expressive than boolean logic, making it both critical to our modern understanding of mathematics, and complicated enough that we can't declare it absolutely true. 

Predicate logic makes use of the same operations and conventions of boolean logic ($\wedge$, $\vee$,$\neg$,$\implies$,$\iff$ and variables), but adds constant symbols, function symbols,  predicate symbols, quantifiers, and a special equality predicate. 

These additions allow us to talk in generalities about entities that live in some universe, for example the natural numbers. 

\begin{defn}[constant symbols]
  Constant symbols name specific entities in the universe.
\begin{example}[constant symbols]
  $0$, $1$, $30$, $10$ are constant symbols referring to natural numbers. 
\end{example}
\end{defn}

\begin{defn}
  Function symbols name functions that take in entities and output an
  entity.

  The arity of a function is the number of arguments it takes, \eg $+$ has arity $2$.
Functions do not reside in the proverbial universe. If we are working in a universe of natural numbers, we will want to refer to $+$. But $+$ is not a number. It belongs in a separate bin.
\end{defn}
\begin{example}
  $+,\times$ are functions of natural numbers. 
\end{example}
\begin{defn}[predicate symbols]
  Predicate symbols name predicts, which take in entities and output true or false.
Like functions, predicates have arity and exist outside of the universe. But predicates are distinct from functions. Predicates output truth values, but functions output elements of the universe (\eg natural numbers). If add two numbers, you can multiply the result by $2$: $(1+3)\times 2$. You cannot, however, say $(1<3)+5$. 
\end{defn}
\begin{example}
  $<,\geq,=$ are predicates of natural numbers
\end{example}

\begin{defn}[quantifier]
  Quantifiers allow us to talk about general elements in the universe. There are two quantifiers: $\forall$, read as `for every', and $\exists$, read as `there exists'.

  Intuitively, $\forall x \phi(x)$ means $\phi(x)$ is true, no matter what $x$ is in the universe, and $\exists x \phi(x)$ means somewhere in the universe, there is an $x$ that satisfies $\phi$. We will make this exact through inference rules later.
\end{defn}
If we refer to Socrates by the constant symbol $s$, we can state the syllogism `Socrates is a man. All men are mortal. Hence, Socrates is mortal' as follows:
\[
\texttt{man?}(s) \wedge \forall x \bigl(\texttt{man?}(x)\implies\texttt{mortal?}(x)\bigr) \implies \texttt{mortal?}(s)
\] 
Recall the statement
\begin{quote}
  Every mouse fears some cat
\end{quote}
We can now state this unambiguously as 
\[
\forall m \bigr(\texttt{mouse?}(m) \implies \exists c (\texttt{cat?}(c)\wedge \texttt{fears?}(m,c)) \bigl)
\]
\ie `for every mouse, we can find a cat which that specific mouse fears.'
\[
\exists c \bigl(\texttt{cat?}(c) \wedge \forall m (\texttt{mouse?}(m) \texttt{fears?}(m,c)\bigr)
\]
\ie `there is one particularly frightening cat all mice fear'.

\begin{defn}[equality]
  Equality is a predicate that always satisfies the following axioms
  \begin{enumerate}
  \item Reflexivity: $\forall x (x=x)$
  \item Substitution: equal elements are interchangeable 
    \begin{trivlist}
    \item for all functions $f$: $\forall x\forall y (x=y\implies f(x)=f(y))$\footnote{Quantifiers range over the universe. But functions don't live in the universe. So we can't write something like $\forall f \forall x \forall y (x=y\implies f(x)=f(y))$. The solution is to treat this condition as a template, or \emph{axiom schema}, and copy it over for each function.}
    \item for predicates $\phi$: $\forall x \forall y\bigl(x=y \implies (\phi(x)\iff \phi(y))\bigr)$
    \end{trivlist}
  \end{enumerate}
\end{defn}

\section[Peano Arithmetic]{peano arithmetic}
I promised this would be sufficient to house modern mathematics. I will show you how the natural numbers can be defined using Peano's axioms \cite{wiki:peano}. 

\renewcommand{\succ}{\texttt{succ}}
\begin{defn}[Peano arithmetic]
  There is a constant called $0$ and a function of one argument called
  $\succ$.  The Peano axioms are as follows:
  \begin{enumerate}
  \item Nothing comes before zero:
    \[
    \label{peano:0}
    \forall x. 0 \neq \succ(x)
    \]
  \item Two naturals are equal if their successors are equal:
    \[
    \label{peano:succ}
    \forall x\forall y.\bigl( \succ (x) = \succ(y) \implies x=y\bigr)
    \]
  \item Induction: if $\Phi$ is a predicate,
    \[
    \label{peano:ind}
    \Phi(0) \wedge \forall x \bigl(\Phi(x)\implies
    \Phi(\succ(x))\bigr) \implies \forall y.\Phi(y)
    \]
    \ie, if $\Phi(0)$ and $\Phi(s)$ implies $\Phi(s+1)$, then
    $\Phi$ must be true for all natural numbers.
  \end{enumerate}
\end{defn}

\begin{defn}[plus]
  \label{defn:peano:+}
  We can define $+$, a function of two inputs, by recursion:
  \begin{align*}
    \forall x (x + 0 &= x) \\
    \forall x \forall y \bigl(x + \succ(y) &= succ(x+y)\bigr)
  \end{align*}
\end{defn}
\section[Deduction]{deduction}
In boolean algebra, when determining the truth of a statement, the worst case scenario will require writing out a large truth table. Not so for predicate logic. Quantifiers range over everything in some possibly infinite universe. Predicates abstract away calculations to something behind the scenes. Without any way to explicitly enumerate everything in the universe, the logician must indicate its content, as well as the behavior of some predicates, with axioms.  

Some axioms, like $\forall x P(x)$ and $\exists x \neg P(x)$ are contradictory, hence nonsensical. One would hope this is just a case of `ask a bad question; get a bad answer'. We would hope that the laws of deduction, which govern what you can actually do with the axioms you choose, are ok. 

% Prior to stating the laws of deduction, we must define the following: 
% \begin{defn}[bound variable]
%   A variable is said to be bound if it is introduced by a quantifier. 
% \end{defn}
% \begin{example}[bound variables]
%   In the expression
%   \[
%   \forall x (\texttt{even?}(x)\implies \exists y . x=2y)
%   \]
%   the variables $x$ and $y$ are bound.  
% \end{example}
% \begin{defn}[free variable]
%   A variable that is not bound is called free. 
% \end{defn}
% \begin{example}[free variable]
%   In 
%   \[
%   \forall y . x\leq 2y
%   \]
%   $y$ is bound but $x$ is free. 
% \end{example}
% \begin{defn}[scope]
%   The scope of a bound variable is the part of the expression where the variable is understood to mean the variable introduced by a quantifier. We can make scope explicit with parenthesis:
%   \[
%   \forall x \bigl(\underbrace{\texttt{cat?}(x)\implies \texttt{mammal?}(x)}_{\textrm{scope}}\bigr)
%   \]
%   When no parenthesis are present, the scope is understood to lie between the quantifier and the first $\implies$ or $\iff$:
%   \[
%   \forall x . \underbrace{x\neq 0\wedge x \leq 3}_{\textrm{scope of $x$}}  \implies \neg \exists y . \underbrace{y > 3}_{\textrm{scope of $y$}}
%   \]
% \end{defn}

For $\wedge,\vee,\neg,\implies,\iff$, deduction works the same as it did for boolean logic:
\begin{trivlist}
\item $x \wedge y$ is true if and only if (iff) $x$ is true and $y$ is true
\item $x \vee y$ is true iff $x$ is true or $y$ is true 
\item $\neg x$ is true iff $x$ is false   
\item $x \implies y$ is true unless $x=1$ and $y=0$  
\item $x\iff y$ is true iff $x$ has the same truth value as $y$  
\end{trivlist}

 
\begin{trivlist}
\item Universal Introduction:
  If we know $\phi(\alpha)$, we can conclude $\forall \alpha . \phi(\alpha)$ so long as $\alpha$ is unused elsewhere.
\item Universal Elimination:
  If we know $\forall \alpha . \phi(\alpha)$ then we can conclude $\phi(\beta)$. 
\item Existential Introduction:
  If we know $\phi(\beta)$ then we can conclude $\exists \alpha \phi(\alpha)$ so long as $\alpha$ is previously unused.
\item Existential Elimination:
  If we know $\exists \alpha \phi(\alpha)$, we can say $\phi(\beta)$ so long as $\beta$ is previously unused. 
\end{trivlist}
Applications of some rules may require renaming variables. This is no problem as long as the meaning of the statement is not changed (\ie two things that used to have different names cannot have the same name). 

\section[Some Proofs]{some proofs}
We are now prepared to prove things about Peano arithmetic:

\begin{theorem}[$+$ is associative]
  The statement 
  \[
  \forall x \forall y \forall z . (x+y)+z = x+(y+z)
  \]
  is deducible from Peano arithmetic and the definition of \nameref{defn:peano:+}.
\end{theorem}
\begin{proof}
  Short version---this is a proof by induction. First, we prove the base case:
   \begin{equation}
    \label{eq:peano:+assoc:base}
    \forall a \forall b .  (a+b) + 0 = a+(b+0)
  \end{equation}
  Equivalently, $\phi(0)$ where
  \[
  \phi(x) = \forall a \forall b . (a+b)+x = a+(b+x)
  \]
  Noting that $y+0=y$, (see the definition of  \nameref{defn:peano:+}), we 
  see \cref{eq:peano:+assoc:base} is, in fact, true. 
 
  Now we prove the inductive step: 
  \begin{equation}
    \label{eq:peano:+assoc:ind}
    \forall x \bigl(\phi(x)\implies \phi(\succ(x))\bigr)   
  \end{equation}
  Pick $a$, $b$, and $c$. 
  Assume $(a+b)+c = a+(b+c)$, which we abbreviate as $a+b+c$. Now consider $(a+b)+\succ(c)$. By definition of \nameref{defn:peano:+}, we see \[(a+b)+\succ(c)=\succ(a+b+c)\] Similarly, \[a+(b+\succ(c))=a+\succ(b+c)=\succ(a+b+c)\]
  hence $(a+b)+\succ(c)=a+(b+\succ(c))$. This proves the inductive step, \cref{eq:peano:+assoc:ind}.
  
  To finish the proof, use the induction axiom. 
\end{proof}

The above proof does not justify each step, but gives enough information to show how deduction works. This is the norm for mathematical proofs. Actually, such a proof is boring when you are studying the mathematics of the natural numbers: it is not hard to convince someone addition is commutative. 

But since we want to understand the very fabric of reason itself, it must be possible to write out this proof in such painstaking detail that a computer, or even the most pedantic of logic professors, can follow it. Such a proof may look like the following: 

\begin{proof}
  Full gory details---First establish the base case, \cref{eq:peano:+assoc:base}
  Consider the statement $(a+b)+0$. By \cref{defn:peano:+}, we know $\forall x . x+0=x$. By universal elimination, deduce $(a+b)+0=a+b$. Now consider
  $a+(b+0)$. By universal elimination again, deduce $b+0=b$. By substitution, deduce $a+(b+0)=a+b$. Then, by transitivity, we know $(a+b)+0=a+(b+0)$. As $a$ and $b$ are totally arbitrary free variables, two universal introductions establish \cref{eq:peano:+assoc:base}.

  Now the inductive step, \cref{eq:peano:+assoc:ind}
  Assume \begin{equation}(a+b)+c=a+(b+c)=a+b+c\label{eq:peano:+assoc:ass}\end{equation}
  Consider the statement $(a+b)+\succ (c)$. From \cref{defn:peano:+}, we know
  \[\forall x \forall y . x+\succ(y)=\succ(x+y)\]
  An analogous quantifier elimination gives $(a+b)+\succ (c) = \succ(a+b+c)$. 
  Another quantifier elimination gives $a+(b+\succ(c))= a+\succ(b+c)$. A final quantifier elimination gives $a+\succ(b+c)=\succ(a+b+c)$. Transitivity gives $(a+b)+\succ(c)=a+(b+\succ(c))$. We can stop assuming \cref{eq:peano:+assoc:ass}, rephrasing our work so far as
  \[
  (a+b)+c=a+(b+c)\implies (a+b)+\succ(c) = a+(b+\succ (c))
  \]
  Now $a$, $b$, and $c$ are arbitrary, so we can use a triple quantifier introduction to prove \cref{eq:peano:+assoc:ind}. 

  As we have shown \cref{eq:peano:+assoc:base,eq:peano:+assoc:ind} we can deduce 
  \begin{equation}
    \label{eq:peano:+assoc:both}
    \phi(0)\wedge \forall x \bigl(\phi(x)\implies \phi(\succ(x))\bigr)
  \end{equation}
  By the induction axiom, 
  \[
  \phi(0)\wedge \forall x \bigl(\phi(x)\implies \phi(\succ(x))\bigr) \implies \forall x \phi(x)
  \]
  Then using boolean logic (\emph{modus ponens}), conclude $\forall z \phi(z)$. Hence, $+$ is associative. This proves the theorem. 
\end{proof}

Ironically, justifying each step results in a proof that is harder for a human to understand. 
\section[The Death of Reason]{the death of reason}
Once mathematicians started actually reading Frege, it was not long until they would dream the impossible dream. Hilbert and his entourage longed for a world in which mathematics had been effectively solved, the right axioms were chosen, and a machine would be built that would write out every true theorem until the end of time. All mathematicians would then be free to sip martinis poolside as an endless tickertape listing all true things would scroll by. \cite{wiki:hilbert-prog}

The mathematicians would have to really read this carefully every now and then to answer a physicist's question. But they discovered by speaking in gratuitous generalities and shouting `the proof is trivial' rather frequently, they could make a pesky physicist disappear, ensuring a veritable utopia (see \url{http://www.smbc-comics.com/index.php?db=comics&id=2675#comic}). Thus French Formalism was born.  

But, one fateful night in 1931, dear reader, a young Kurt Gödel dealt a fatal blow to the Hilbertian dream of mathematical rapture. He was merely engaging in the logician's favorite pass-time: making things mercilessly meta. As many a logician and patron of webcomics know, making things meta may make a mess, and a major mess it made moreover. To his merit, this metamethematical misfortune marks the makings of a modern mathematical maturity, one more meaningful than a mere mirage marred by the multitude of mathematical models and their unmarried meandering truths. Any system of mathematics sufficiently sophisticated to serve as semantics for something like Peano's arithmetic suffers from a malady said to be incompleteness. See, m'lady, such a malady means more than several true-seeming statements may lay beyond reach of the most surefire machines of Hilbert's dreams.

\subsection{berry's paradox}
There are only so many numbers you can name in under eleven words, finitely many in fact. So consider the smallest positive integer not nameable in under eleven words. Surely it exists; every nonempty set of naturals has a least number (exercise for the reader!). When considering how many words it takes to name this number, 10 is right out. Except, of course, I named it in 10 words: `the(1) smallest(2) positive(3) integer(4) not(5) nameable(6) in(7) under(8) eleven(9) words(10)'.  

\subsection{incompleteness} 
This proof is due to George Boolos. Gödel's proof used the liar's paradox (this sentence is false), which leads to a longer proof.\cite{wiki:incomplete-sketch} In both cases, the genius of the proof lies in encoding these paradoxes into Peano arithmetic. 

\begin{defn}[Gödel numbering]
  A Gödel numbering is a lossless encoding of first order logical statements in the natural numbers. Many such encodings exist. For our purposes, any one will work. 
\end{defn}
Once we pick a Gödel numbering, Peano arithmetic becomes self-referential.  

\begin{defn}[names]
  A formula $\phi$ names the number $n$ iff $\forall x (F(x)\iff x=n)$. 
\end{defn}
Now we define some predicates:
\newcommand{\namelen}{\texttt{nameln?}}
\begin{defn}[\namelen]
  $\namelen(x,y)$ is true iff $x$ can be named by a statement containing $y$ symbols. We can define this purely in terms of arithmetic operations via our Gödel numbering. 
\end{defn}
\newcommand{\namesmaller}{\texttt{smaller?}}
\begin{defn}[\namesmaller]
  \[
  \namesmaller(x,y)\iff \exists z .z < y \wedge \namelen(x,z)
  \]
  \ie $\namesmaller(x,y)$ iff $x$ can be named by a statement with length less than $y$. 
\end{defn}
\newcommand{\minnodef}{\texttt{nodef?}}
\begin{defn}[\minnodef]
  \[
  \minnodef(x,y)\iff \neg \namesmaller(x,y) \wedge \forall a \bigl(a < x \implies \namesmaller(a,y)\bigr)
  \]
  \ie $\minnodef(x,y)$ iff $x$ is the smallest number that cannot be named in $y$ symbols. 
\end{defn}
\newcommand{\berry}{\texttt{berry?}}
\begin{defn}[\berry]
  \[
  \berry(x) \iff\exists y. y=\succ^{10}(0)\times \succ^{k}(0)\wedge \minnodef(x,y)
  \]
  where $\times$ is multiplication, $\succ^n$ is $n$ applications of $\succ$, and $k$ is the length of $\minnodef(x,y)$.
  This encodes Berry's paradox in Peano arithmetic. 
\end{defn}
\begin{theorem}[Gödel's first incompleteness]
  There is some statement that cannot be proven true or false in Peano arithmetic.\footnote{Assuming Peano arithmetic is consistent (free of contradictions).}
\end{theorem}
\begin{proof}
  We will construct such a statement. Let $N$ be the smallest number that cannot be named in fewer than $10k$ symbols. Define 
  \begin{equation}
    \label{eq:godel-sentence}
    \forall x \bigl(\berry (x) \iff x= \succ^N(0)\bigr)
  \end{equation}
  As $\berry(x)$ places a minimality condition on $x$ via the $\minnodef(x,y)$ term, at most one element can satify $\berry(x)$. Thus, 
  a proof of \cref{eq:godel-sentence}'s negation amounts to showing $N$ does not satisfy $\berry$. A contradiction, as $N$ cannot be named in fewer than $10k$ symbols. A proof of \cref{eq:godel-sentence} would prove $\berry$ names $N$ in fewer than $10k$ symbols, again a contradiction. 
\end{proof}
\chapter[Set theory]{set theory}
While Frege was laying the groundwork for a new ontology of mathematics, Cantor proposed a controversial theory of infinite sets. Today, this theory is used to give us a big enough universe to do almost all of mathematics.

While investigating an unsolved problem in analysis, Cantor found a solution that required numbers beyond infinity. 
The mathematics of his time had no way to make sense of such large numbers. Captivated, Cantor turned his focus to making the infinite precise. Conventional wisdom said this was not possible. Cantor believed that, not only was this possible, but progress in mathematics required it. 

To lay foundations on which he could build his theory the infinite, Cantor invented set theory. These sets were just collections of objects that were possibly infinite. Next, he looked for a way to compare the size of infinite objects. Once he accomplished this, he was able to show that some infinite sets were smaller than others. This idea was met with both derision and celebration. Gradually, mathematics came to see the necessity of Cantor's insights. Today, you can't get far in mathematics without running into sets. 
\section[Naïve set theory]{naïve set theory}
\begin{defn}[naïve set]
  A set is an unordered collection of mathematical objects. A set may be infinite. Two sets are the same if they have the same elements. 
\end{defn}
\begin{defn}[member]
  We say $y$ is an element of the set $X$ if $y$ contains $X$. This is written $y\in X$. So $\{1,2,3\}=\{2,1,3\}=\{3,2,1\}$ because the elements are the same and order doesn't matter. 
\end{defn}
\begin{defn}[subset $\subseteq$]
  A set $X$ is a subset of $Y$, written $X\subseteq Y$, if and only if (iff) every element of $X$ is also an element of $Y$. 
\end{defn}
\begin{example}
  The collection $\N$ is a set of all natural numbers. It is infinite. The set of even numbers $E$ is a subset of $\N$, \ie $E\subseteq \N$. 
\end{example}
One may take unions $(\cup)$ and intersections $(\cap)$ of sets. 

Now we would like to know how to compare the sizes of infinite sets. 
\begin{defn}
  Two sets $X$ and $Y$ are in a $1$-to-$1$ correspondence if we can match each element of $X$ with exactly one element of $Y$ and have no elements in $Y$ left over. 
\end{defn}
\begin{example}
  The sets $\{1,2,3\}$ and $\{5,6,7\}$ are in a $1$-to-$1$ correspondence, namely 
  \begin{align*}
    1 &\mapsto 5 \\
    2 &\mapsto 6 \\
    3&\mapsto 7
  \end{align*}
  There are other $1$-to-$1$ correspondences between these sets.  
\end{example}
\begin{example}
  The set $\N$ (natural numbers) is in a $1$-to-$1$ correspondence with $E$ (even numbers). 
  \begin{align*}
    0 &\mapsto 0 \\
    1 &\mapsto 2 \\
    2 &\mapsto 4 \\
    &\vdots \\ 
    n & \mapsto 2n \\
    &\vdots 
  \end{align*}
\end{example}
\begin{defn}[size]
  Two sets have the same size iff there is a $1$-to-$1$ correspondence between them. 
\end{defn}
So we can see there are as many even numbers as there are natural numbers. Likewise, there are as many even numbers as odd numbers. 
\begin{theorem}
  There set of natural numbers ($\N$) and integers ($\Z$) have the same size. 
\end{theorem}
\begin{proof}
  \[
  \begin{matrix}
    \N:& 0 & 1 & 2 & 3 & 4 & \dots \\
       & \updownarrow & \updownarrow& \updownarrow&\updownarrow&\updownarrow& \\
    \Z:& 0 & 1 & -1 & 2 & -2 & \dots 
  \end{matrix}
  \]
\end{proof}

\begin{theorem}
  The set $\N$ (natural numbers) has the same size as $\N^2$ (pairs of natural numbers). 
\end{theorem}
\begin{proof}
  Lay out $\N^2$ in the following grid: 
  \[
  \begin{matrix}
    &&&&& (0,0)&  \\
    &&&&(0,1)& & (1,0) \\
    &&&(0,2) && (1,1) && (2,0)  \\
    &&(0,3) && (1,2) && (2,1)&& (3,0) \\
    &(0,4) && (1,3) && (2,2) && (3,1) && (4,0) \\
    \dots && \dots && \dots && \dots && \dots  
  \end{matrix}
  \]
  Reading off each row from the top gives the following $1$-to-$1$ correspondence:
  \[
  \begin{matrix}
    \N:& 0 & 1 & 2 & 3 & 4 & \dots \\
       & \updownarrow & \updownarrow& \updownarrow&\updownarrow&\updownarrow & \\
    \N^2:& (0,0) & (0,1) & (1,0) & (2,0) & (1,1) & \dots 
  \end{matrix}
  \]
\end{proof}

\begin{theorem}
  The set $\N$ (natural numbers) is the same size as $\Q$ (rational numbers)
\end{theorem}
\begin{proof}
  Lay out $\Q$ in the following grid, writing only the fractions that are in simplest form: 
  \begingroup
  \renewcommand*{\arraystretch}{2.2}
  \[
  \begin{matrix}
    0 \\
    \dfrac{-1}{1} & \dfrac 11\\
    \dfrac{-1}{2} & \dfrac{1}{2}\\
    \dfrac{-2}{3} & \dfrac{-1}3 & \dfrac 1 3 & \dfrac 2 3 \\
    \dfrac{-3}4 & \dfrac{-1}4 & \dfrac 1 4 & \dfrac 3 4 \\
    \dfrac{-4}5 & \dfrac {-3}5 & \dfrac {-2}5 & \dfrac {-1}5 & \dfrac 15 &\dfrac 25& \dfrac 35& \dfrac 45 \\
    \vdots 
  \end{matrix}
  \]
  \endgroup
  Reading off from the top gives a $1$-to-$1$ correspondence: 
  \[
  \begin{matrix}
    \N:& 0 & 1 & 2 & 3 & 4 & \dots \\
       & \updownarrow & \updownarrow& \updownarrow&\updownarrow&\updownarrow & \\
    \Q:& 0 & -1/1 & 1/1 & -1/2& 1/2 & \dots 
  \end{matrix}
  \]
\end{proof}

\begin{defn}[countably infinite]
  If a set has the same size as the natural numbers, it is countably infinite. 
\end{defn}

\begin{theorem}[uncountably infinite sets exist]
  The set $2^\N$ of infinite binary strings\footnote{This notation is chosen because of the convention $2=\{0,1\}$. Then $2^\N$ is the set of infinite sequences of $1$ or $0$.} is uncountable.
\end{theorem}
\begin{proof}
  Suppose $2^\N$ is countably infinite. Then there is some $1$-to-$1$ correspondence between $2^\N$ and $\N$. We can think of this as a numbered list of all the elements of $2^\N$. For argument's sake, lets say this list is 
  \[
  \newcommand{\markify}[1]{\color{blue}{#1}}
  \begin{matrix}
    0 & \mapsto & \markify 0 & 0 & 0 & 0 & 0 &  \dots \\
    1 & \mapsto & 1 & \markify 0 & 0 & 0 & 0 &  \dots \\
    2 & \mapsto & 0 & 1 & \markify 1 & 0 & 0 &  \dots \\
    3 & \mapsto & 0 & 0 & 1 & \markify 0 & 1 & \dots \\
    4 & \mapsto & 0 & 1 & 0 & 1 & \markify 1 & \dots  \\
    \vdots && \vdots & \vdots & \vdots & \vdots& \vdots & \markify \ddots
  \end{matrix}
  \]
  Now consider the diagonal entry, in this case
  \[
  0,0,1,0,1\dots
  \]
  Negating each term gives
  \[
  x=1,1,0,1,0\dots
  \]
  As $x$ is an infinite sequence of $0$ and $1$s, we know $x\in2^\N$. We are assuming every element in $2^\N$ will show up eventually in our list; thus $x$ must eventually show up. But $x$ cannot be entry $0$ because their $0$-th terms differ. Likewise, $x$ cannot be the entry $1$ because their $1$-th terms differ. In general, $x$ cannot be element number $M$ because the $M$-th terms will differ. So, in fact, $x$ does not show up anywhere in our list, a contradiction. 
 
  I gave specific examples for clarity. But for any list of elements of $2^\N$, we can find an $x$ by negating the diagonal. Then the argument that $x$ cannot be the first, second, third,\dots$M$-th term still works, regardless of the specific values. There is no $1$-to-$1$ correspondence between $M$ and $2^\N$ because such a correspondence would lead to a contradiction. 
\end{proof}
This is the first notable result of the chapter. When Cantor discovered this, it showed the world infinite sets could be understood mathematically, and infinite sets were more complicated than they appeared. You may wonder if there are sets strictly bigger than $2^\N$. There are. In fact, there are infinitely many. 

\section[Russell's paradox]{russell's paradox}
Cantor's results were initially met with skepticism. Mathematicians found his results so counter-intuitive that some suspected they were meaningless. Today you would be hard-pressed to find a mathematician who did not believe Cantor's results, but Cantor's work was free of error. In fact, philosopher-mathematician-politician Bertrand Russell found a glaring error in Cantor's work, known as Russell's paradox: 

Consider the set
\[
R= \{x: x\notin x\}
\]
\ie the set of sets that don't contain themselves. Russell then asked, does $R$ contain itself ($R\in R$)? Suppose it does. By definition of $R$, all sets in $R$ must not contain themselves. Then $R\notin R$, a contradiction. Suppose $R\notin R$. Then, as $R$ does not contain itself, by definition of $R$, we can conclude that in fact $R\in R$, a contradiction. 

This shocked Frege, who was trying to construct all of mathematics through a prototype of what is now known as predicate logic and set theory. But mathematicians persevered in their quest to formalize mathematics. Bertrand Russell, along with his mentor Alfred N. Whitehead, spent a decade trying to lay the logical foundations of mathematics in a way that avoided paradoxes like this. In particular, they were looking for a foundation that was \emph{consistent}---free of contradictions---and \emph{complete}---everything can be proved true or false. 

To avoid Russel's paradox, they built their mathematical universe in layers. In each layer, definitions can only refer to things defined in the layers below. In this system, there is no way to define the set of all sets that contain themselves. They could not show this theory, which they laid out in \emph{Principia Mathematica}\footnote{Named after Newton's book.} was either consistent or complete.\footnote{Gödel would later show both goals were impossible.} \emph{Principia} is so complicated it takes over 360 pages to prove $1+1=2$\cite{principia}. 

In practice, the layers in \emph{Principia} are superfluous for most mathematics. In fact, for the purposes of this book, we will only need at most two such layers. Usually we will be able to get away just by providing axioms that limit the definitions of sets enough to prevent the paradoxes Russell \& Whitehead feared. 

\section[Axiomatic set theory]{axiomatic set theory}
\newcommand{\zfc}{\textsc{zfc}\xspace}
In mathematics today, the conventional foundation is \emph{Zermelo-Fraenkel Set theory with the Axiom of Choice} (\zfc). To prove a theorem, you show that it can be proved by the deduction laws for first order logic starting with the axioms of \zfc. Of course, this is rarely done in practice; such proofs would be incomprehensible to most people. We will not prove things from the axioms of \zfc. In fact, we will only explicitly use some of \zfc's axioms. But, underneath our proofs, will be the implication that this proof can be written out fully in a way a proof-checking computer would agree the conclusions follow from \zfc's axioms. 

We will define \zfc as a first-order theory. There is a constant symbol $\emptyset$, which is the empty set. There is a predicate symbol $\in$ for membership, \ie $x\in y$ iff $x$ is an element of $y$. The axioms are as follows:
\begin{enumerate}
\item Extensionality \\
  Two sets are the same if they have the same elements. In other words, everything you need to know about a set can be determined by looking at its elements.
  \[
  \forall x \forall y  \bigl(\forall z(z\in x \iff z \in y)\implies (x=y)\bigr)
  \]
  \label{extensionality}
\item Regularity \\  
  Every nonempty set is disjoint from at least one of its elements
  \[
  \forall x (x\neq \emptyset \implies \exists y . y\in x \wedge y\cap x = 0)
  \]
  This prevents sets from containing themselves. \label{zfc:reg} 
\item Specification\label{zfc:spec} \\ 
  For any set $X$, we can find a subset containing exactly those elements of $X$ that satisfy a predicate of our choosing. If we call this predicate $\Phi$, then
  \[
  \forall X \exists Y \forall z  (z\in X \wedge \Phi(z) \iff z\in Y)
  \]
  Sets constructed with this axiom will often be written in set builder notation:
  \[
  \{x\in \N: \exists k \in \N \textrm{ such that } x=2k\}
  \]
  is the set of even natural numbers.
\item Pairing \\ 
  For all $x$ and $y$, there is a set which contains both of them. 
  \[
  \forall x \forall y \exists Z .x\in Z \wedge y \in Z
  \]
  We will not use this axiom explicitly, but we will always assume this can be done. 
\item Union \\
  You can take unions, even if they're infinite. 
  For any set of sets $X$, there is a set that accumulates the contents of every element of $X$. This may be written
  \[
  U=\bigcup X
  \]
  or
  \[
  U=\bigcup_{A\in X} A
  \]
  or more frequently, by writing $X=\{X_i:i\in I\}$ where $I$ is a set of indexes,
  \[
  U=\bigcup_{i\in I} X_i
  \]
  The phrasing in terms of a set of sets is awkward, but this allows for infinite unions, which are rather useful. 
  
  The first order sentence is
  \[
  \forall X \exists U \forall Y \bigl(Y\in U \implies \forall z ( z\in Y \implies z \in U)\bigr)
  \]
\item Replacement \\ 
  Suppose the predicate $\Phi(x,y)$ encodes a function, \ie 
  \[
  \forall x \exists! y \Phi(x,y)
  \]
  where $\exists!y$ means there is exactly one $y$ with the required property. 
  
  Then this axiom states the image of a set $X$ under the function encoded by $\Phi$ is also a set. We can \emph{replace} $X$ with its image $Y$ to get a set.
  \[
  \forall X \exists Y \forall x \forall y (x\in X \wedge \Phi(x,y) \implies y\in Y)
  \]
  We will not make explicit use of this. 
\item Infinity \\
  The natural numbers exist. Specifically, we say $0=\emptyset$. Also, let $\succ(x)=x\cup \{x\}$. Then
  \[
  \exists \N (n\in \N\implies \succ(n)\in\N)
  \]
  Once we have the natural numbers, we can create all the infinite sets we want, but there is no way to get to this point from the other \zfc axioms. 
\item Powerset \\
  The powerset of $X$, denoted $2^X$, is the set of subsets of $X$. This axioms guarantees that the powerset of set $X$ is also a set. 
  \[
  \forall X \exists 2^X . \forall z (z\subseteq X \iff z \in 2^X)
  \]
\item Choice \\
This is the most famous and controversial of the \zfc axioms. It says, for any set of nonempty sets, $X$, we can assume there exists a corresponding \emph{choice function}. If we write $X$ in index notation, $X=\{X_i:i\in I\}$, then a choice function is some $f$ such that $f(X_i)\in X_i$. In other words, we can assume there is a way to pick an element from each set in $X$. We defer the first-order sentence stating this axiom until we have talked about functions.  
 \label{zfc:choice}
\end{enumerate}
The work we did before can be formalized in this system:
\begin{defn}[ordered pair]
 \[(x,y)=\{\{x\},\{x,y\}\}\]
 This way, we can distinguish between
 \[
 (x,y)=\{\{x\},\{x,y\}\}
 \]
 and
 \[
 (y,x)=\{\{y\},\{x,y\}\}
 \]
\end{defn}
\begin{defn}[ordered tuple]
  We define the ordered tuple
  \[
  (x_0,\dots,x_n)= \{(0,x_0),(1,x_1),\dots,(n,x_n)\}
  \]
\end{defn}

\begin{defn}[cartesian product]
  Suppose $X$ and $Y$ are sets. Their cartesian product, denoted $X\times Y$, is the set of ordered pairs $(x,y)$ where $x\in X$ and $y\in Y$. 
\end{defn}

\begin{defn}[function]
  A function $f$ from $X$ to $Y$, or 
  \[
  f: X\to Y
  \]
  is a subset of $X\times Y$ where each $x$ in $X$ is paired with one and only one $y$ in $Y$, \ie
  \[
  \forall x \bigl(x\in X \implies\exists! y. y\in Y \wedge (x,y)\in f\bigr)
  \]
  The set of functions from $X$ to $Y$ is denoted by  $Y^X$. 
\end{defn}

\begin{defn}[$1$-to-$1$ correspondence]
  A $1$-to-$1$ correspondence between $X$ and $Y$ is a function $f:X\to Y$ with the property that each $y\in Y$ is the image of exactly one $x\in X$, \ie 
  \[
  \forall y (y\in Y\implies \exists! x . f(x)=y)
  \]
\end{defn}

\section[To infinity and beyond]{to infinity and beyond}
At the opening of this chapter, you were promised we'd count past infinity. But first, a quick detour through order theory: 
\begin{defn}[well-ordered]
  \label{def:well-ordered}
  A set $X$ with an order $\leq$ is well-ordered iff every nonempty subset of $X$ has a least element. 
\end{defn}
\begin{example}
  The set $\N$ with order $\leq$ is well-ordered. To see this, note that a natural number is a set of all the natural numbers before it.
  \begin{align*}
    0 &= \{\} \\
    1 &= \{0\}\\
    2 &= \{0,1\}\\
    3 &= \{0,1,2\}\\
    \vdots 
  \end{align*}
  If $X$ is a nonempty subset of $\N$,
  \[
  \bigcap_{n\in X} n 
  \]
  gives the set of all numbers smaller than everything in $\N$, \ie its least element. 
\end{example}
\begin{example}
  The set $\Z$ (integers) with order $\leq$ is not well-ordered. The easiest way to show this is to consider $\Z$, a subset of $\Z$. There is no smallest integer, so this subset has no least element. 
\end{example}
Each natural number is finite. But we can define an infinite number, one that is greater than all natural numbers.
\begin{defn}[\omega]
  Define
  \[
  \omega=\N
  \]
  Define $x<y$ if $x\in y$. Then $\omega$ is greater than every natural number. 
\end{defn}
This new number $\omega$ cannot be a successor of a natural number, as it is not a natural number. But we can say
\[
\succ(\omega) = \omega \cup \{\omega\} = \N \cup \{\omega\}
\]
\begin{defn}[ordinals]
  We define the Von Neumman ordinals, $\ord$, by saying each ordinal is the well-ordered set of all smaller ordinals. There are two types of ordinals:
  \begin{itemize}
  \item $\alpha$ is a \emph{successor ordinal} if there is some $\beta \in \ord$ such that $\alpha=\succ(\beta)$. 
  \item $\alpha$ is a \emph{limit ordinal} if it is not a successor ordinal. 
  \end{itemize}
  Here, too, taking the intersection of all elements in a non-empty set gives their least element. 
\end{defn}
\begin{theorem}[$\ord$ is too big to be a set]\label{th:ord-proper-class}
  $\ord$ is too big to be a set. We call such an object a \emph{class}.\footnote{There are axioms for class theories analogous to \zfc, but we will not need these, as we will not use many classes}
\end{theorem}
\begin{proof}
  Suppose $\ord$ were a set. Then $\ord$ would also be an ordinal. But by the axiom of regularity (\ref{zfc:reg}), $\ord$ cannot contain itself. 
\end{proof}
\begin{theorem}[transfinite induction]\label{def:trans-ind}
  A predicate $\Phi$ is true for all ordinals if
  \begin{trivlist}
  \item Base case: $\Phi(0)$
  \item Inductive step: If, for all $\alpha$ less than some $\beta$, $\phi(\alpha)$, we can conclude $\phi(\beta)$. 
  \end{trivlist}
\end{theorem}
\begin{proof}
  Suppose not. Then there is some ordinal $\beta>0$ that is the smallest ordinal for which $\phi$ is false. Then, for all $\alpha < \beta$, we know $\phi(\alpha)$. But, by assumption, the inductive step holds; therefore we conclude $\phi(\beta)$, a contradiction. 
\end{proof}
\section[The axiom of choice]{the axiom of choice}
We introduce the shorthands $\forall x \in X \phi(x)$ for $\forall x . x\in X \implies \phi(x)$ and $\exists x \in X \phi(x)$ for $\exists x . x\in X \wedge \phi(x)$. 
\begin{defn}[axiom of choice] \label{choice}
  For any set of sets $X$, if $\emptyset \notin X$ there is a choice function $f:X\to \bigcup X$ such that $f(X_i)\in X_i$, or 
\begin{equation}
\forall X \left(\emptyset \notin X \implies \exists f \in \left(\bigcup X\right)^X \forall A \in X . f(A)\in A\right)
\label{eq:choice}
\end{equation}
\end{defn}
The axiom of choice allows you to make an infinite number of arbitrary choices. In the general case, a description of a choice function or the elements it returns are not possible. This means \zfc contains elements whose existence we assert, although we can never find them. This is the heart of the early 20th century controversy over the axiom of choice. 

\begin{defn}[infinite cartesian product]
  Let $I$ be an index set. If each $X_i$ is a set, we can define their product,
  \[
  \prod_{i\in I} X_i=\left\{f\in\left(\bigcup_{i\in I} X_i\right)^I: f(i)\in X_i\right\}
  \]
  \ie the set of functions from $I$ to $\bigcup_{i\in I} X_i$ such that $f(i)\in X_i$. 
\end{defn}
We can restate the axiom of choice in terms of the above.
\begin{theorem}
  Let $I$ be an index set. Suppose each $X_i$ is a nonempty set and $X_i\neq X_j$ whenever $i$ and $j$ are distinct. Then the \nameref{choice} is equivalent to
  \begin{equation}
    \prod_{i\in I} X_i\neq \emptyset
    \label{eq:nonempty-prod}
  \end{equation}
\end{theorem}
\begin{proof}
The set of choice functions on $\bigcup_{i\in I} X_i$ is 
  \[
  C=\left\{f\in \left(\bigcup_{i\in I} X_i\right)^X: f(X_i)\in X_i\right\}
  \]
  whereas
  \[
  P=\prod_{i\in I} X_i = \left\{f\in\left(\bigcup_{i\in I} X_i\right)^I: f(i) \in X_i\right\}
  \]
  There is a $1$-to-$1$ correspondence between $P$ and $C$ given by converting from $f$ such that $f(X_i)\in X_i$ to an $f'$ such that $f'(i)=f(X_i)\in X_i$. Setting $f'(i)=f(X_i)$ gives this conversion
\end{proof}

\begin{theorem}[well-ordering theorem]\label{th:well-ordering}
  For every set $X$ there is some order $\leq$ that makes $X$ \nameref{def:well-ordered}. 
\end{theorem}
\begin{proof}
  We will define a $1$-to-$1$ function from a subset of $\ord$ to $X$ by \nameref{def:trans-ind}. Let $f$ be a choice function for $2^X$. We will be using $f$ to pull out successive elements of $X$, from which we will define a well-order. Define
  \begin{align*}
    X_0 &= X \\
    x_0 &= f(X_0)
  \end{align*}
  For $\alpha$ an ordinal, define 
  \begin{align*}
    X_{\succ(\alpha)} &= X_\alpha-\{x_\alpha\} \\
    x_{\succ(\alpha)} &= f(X_\alpha)
  \end{align*}
  For $\beta$ a (nonzero) limit ordinal, define 
  \begin{align*}
    X_\beta &= \bigcap_{\alpha < \beta} X_\alpha \\
    x_\beta &=f(X_\beta)
  \end{align*}
  Let $\Omega$ be the smallest ordinal such that $X_\Omega=\emptyset$. The ordinal $\Omega$ must exist, otherwise we have defined a $1$-to-$1$ function from $\ord$ to $X$, which is impossible by \nameref{th:ord-proper-class}. Now we can say the function $n\mapsto x_n$ is a $1$-to-$1$ function from $\Omega$ to $X$. Finally, we define the order by 
  \[
  x_n \leq x_k \iff n \leq k
  \]
\end{proof}
This version of the axiom of choice seems intuitively true. 
\begin{theorem}[well-ordering theorem \iff\ choice]
  Suppose instead of assuming the axiom of choice, we use the axioms of Zermelo-Fraenkel set theory and \nameref{th:well-ordering}. Then we can prove the axiom of choice. 
\end{theorem}
\begin{proof}
  Consider the set of sets $X$. Suppose $\emptyset\notin X$. Let $\leq$ be a well-ordering on $\bigcup X$, which exists by hypothesis. Define a choice function for $X$ by sending the set $A\in X$ to its least element under $\leq$. 
\end{proof}
\section[Number systems]{number systems}
We are almost ready to start defining the familiar number systems in terms of set theory. In fact, we can do so right now, but we will have no way of telling our theory that $1/2$ and $2/4$ are identical objects. To do this, we need to split our representations of a number system into \emph{equivalence classes}. 
\begin{defn}[equivalence relation]
  The $2$-ary predicate $\sim$ on the set $X$ is an equivalence relation iff it satisfies
  \begin{enumerate}
  \item Reflexivity: $\forall x \in X.x\sim x$
  \item Symmetry: $\forall x,y\in X (x\sim y \implies y\sim x)$
  \item Transitivity: $\forall x,y,z\in X (x\sim y \wedge y\sim z\implies x\sim z)$
  \end{enumerate}
\end{defn}
\begin{defn}[equivalence classes]
  Suppose $X$ is a set and $\sim$ is an equivalence relation on $X$. Then $X/\sim$, equivalence classes of $X$ mod $\sim$, is a set of subsets of $X$ such that 
  \begin{enumerate}
  \item Every $x\in X$ is in some $A \in X/\sim$. 
  \item For all $A\in X$, all elements of $A\in X/\sim$ are $\sim$-equivalent.
  \item For all $A,B\in X$, there are no $a\in A$, $b\in B$ such that $a\sim b$. 
  \end{enumerate}
\end{defn}
\subsection[Integers]{integers}
We can represent integers as pairs of natural numbers $(a,b)$ which will represent the number we think of as $a-b$. 
\begin{defn}[\sim]
  \[
  (a,b)\sim(x,y) \iff a+x=b+y
  \]
\end{defn}
Then 
\begin{defn}[$\Z$]
  Define the integers
  \[
  \Z=\N^2/\sim
  \]
  Using equivalence classes lets us treat different representations of the same number as equal. If we just looked at pairs in $\N^2$, we could say $(0,3)\equiv (1,4)$, but not $(0,3)=(1,4)$ as they are different pairs. The solution? Define an integer as the set of all pairs that represent it. 

  Define addition by its action on pairs $(a,b)$ and $(x,y)$:
  \[
  (a,b)+(x,y)=(a+x,b+y)
  \]
  or, in a more suggestive notation, 
  \[
  (a-b)+(x-y)=\bigl(a+x-(b+y)\bigr)
  \]
  negation by
  \[
  -(a-b)=(b-a)
  \]
  and multiplication by
  \[
  (a - b)\cdot(x-y)=\bigl(ax +by - (ay+bx)\bigr)
  \]
  Technically, we defined $+$, $-$ and $\cdot$ for pairs in $\N^2$. To see these define operations on $\Z=\N^2/\sim$, check that equivalent inputs lead to equivalent outputs. 
\end{defn}
\subsection[Rationals]{rationals}
Rationals will be represented by pairs of an integer (numerator) and a positive integer (denominator). Let $\Z^+$ denote strictly positive integers. 
\begin{defn}[\sim]
  For pairs in $\Z\times \Z^+$,
  \[
  (a,b)\sim(x,y) \iff ay=bx
  \]
  or, in a more suggestive notation, 
  \[
  \frac a b \sim \frac x y \iff ax = by
  \]
\end{defn}
Then
\begin{defn}[$\Q$]
  \[
  \Q=(\Z\times\Z^+)/\sim
  \]
  where we define addition as
  \[
  \frac ab + \frac xy = \frac{ay+bx}{by}
  \]
  negation as
  \[
  -\frac ab = \frac{-a}b
  \]
  multiplication as
  \[
  \frac ab \frac xy = \frac {ax}{by}
  \]
  and the multiplicative inverses as
  \[
  \left(\frac ab\right)^{-1} = \frac ba
  \]
\end{defn}
\subsection[Reals]{reals}
Pythagoras believed all numbers were rational. This was key in his world view. But one day a member of his math-cult stumbled on an irrational number. Pythagoras was so appalled, legend has it he killed the discoverer to suppress this discovery. But mathematics cannot be suppressed.
\begin{figure}[h]
  \centering
  \caption{Irrational numbers in geometry}
  \begin{asy}
    size(150);
    pen squares = gray(.6);
    draw((0,0)--(1,0)--(1,-1)--(0,-1)--cycle,squares);
    draw((0,0)--(0,1)--(-1,1)--(-1,0)--cycle,squares);
    draw((1,0)--(0,1)--(1,2)--(2,1)--cycle,squares);

    draw((0,0)--(1,0)--(0,1)--cycle);
    fill((0,0)--(1,0)--(0,1)--cycle,gray(.8));
    label("$1$",(.5,-.2));
    label("$1$",(-.1,.5));
    label("$\sqrt 2$",(.6,.7));
  \end{asy}
\label{fig:irrational-geom}
\end{figure}
As seen in \cref{fig:irrational-geom}, simple geometry confronts us with irrational numbers.
\begin{theorem}[$\sqrt 2$ is irrational]
  There is no pair of integers $a,b$ where $a/b$ is in simplest form that satisfy 
  \begin{equation}
  \left(\frac a b\right)^2=2
  \label{eq:sqrt-2}
\end{equation}
\end{theorem}
\begin{proof}
  Suppose there are integers $a,b$ with $a/b$ in simplest form that satisfy \cref{eq:sqrt-2}. Then
  \begin{align}
    \frac {a^2} {b^2}&=2\label{eq:dist-sqrt-2}\\
    a^2&=2b^2\nonumber
  \end{align}
  so $a$ is even. Then there is some $n\in \Z$ such that $a=2n$. Substituting $2n$ for $a$ in \cref{eq:dist-sqrt-2} gives
  \[
  \frac {(2n)^2}{b^2}=\frac {4n^2}{b^2}=2
  \]
  Solving for $b$ gives
  \[
  b^2=2n^2
  \]
  so $b^2$ is even. Recall we're assuming $b$ is an integer. The only way for $b^2$ to be even when $b$ is an integer is for $b$ to be even. Then $a$ and $b$ are both even, so $a/b$ is not in simplest form, a contradiction.
\end{proof}
This tells us $\Q$ is missing numbers. But so far we only know for sure $\Q$ is missing numbers involving $\sqrt 2$. What about numbers like $\pi$ or $e$ that have no obvious connection to $\sqrt 2$? 

Rational numbers can approximate $\sqrt 2$:
\begin{theorem}[babylonian method]
  \label{th:babylon}
  Let $x$ be an approximation for $\sqrt S$. We can get a slightly better approximation $x'$ by
  \[
  x'=\frac{x+S/x}{2}
  \]
\end{theorem}
\begin{proof}
Let $\varepsilon$ the approximation's error, \ie $\varepsilon = x-\sqrt S$.
  In other words,
  \begin{align}
    S &= (x+\varepsilon)^2 \nonumber\\
    S &= x^2+2x\varepsilon + \varepsilon^2 \nonumber\\
    S&= x^2+\varepsilon(2x+\varepsilon) \nonumber\\
    \varepsilon &= \frac{S-x^2}{2x+\varepsilon}\label{bab-error-exact}
  \end{align}
  If $\varepsilon$ is small enough, we can approximate $2x+\varepsilon \approx 2x$. Then \cref{bab-error-exact} is approximately
  \[
  \varepsilon \approx \frac {S+x^2}{2x}
  \]
  Now we improve our approximation:
  \begin{align*}
    x' &= x+\varepsilon \\
    x' &= x+ \frac{S-x^2}{2x} \\
    x' &= \frac{2x^2+S-x^2}{2x}=\frac{S+x^2}{2x} \\
    x' &= \frac{x+S/x}{2}
  \end{align*}
\end{proof}
\begin{cor}
  We can find a sequence of rational numbers that approximates $\sqrt{2}$.
\end{cor}
\begin{proof}
The \nameref{th:babylon} will give a rational number for $x'$ when $x$ and $S$ are rational. 
\end{proof}
\begin{example}
  If we start with the initial guess $\sqrt 2 \approx 1$, the \nameref{th:babylon} will give 
  \begin{align*}
    x_0&=1 \\
    x_1 &= \frac{1+2/1}2 = 1.5 \\
    x_2 &= \frac{1.5+2/1.5}2 = 1.41\bar6 \\
    x_3 &= \frac{1.41\bar6 +2/1.41\bar6}2 = 1.4142156862\dots \\
    x_4 &= \frac{1.4142156862\dots+2/1.4142156862\dots}2 = 1.414213562\dots \\
        \vdots
  \end{align*}
\end{example}
Continuing in this fashion, we can construct an infinite sequence $x_0,\dots,x_n\dots$ of rationals that approaches $\sqrt 2$. But we still haven't figured out a definition of irrational numbers in \zfc. We want to find a property that says $x_0,\dots,x_n\dots$ looks like an approximation of some number. The key observation is the larger the $n$, the smaller the adjustments you have to make to get more precision. This is the inspiration for the following definition.
\begin{defn}[approximation-like]\label{def:cauchy}
  \begin{sloppypar}
    A sequence $x_n$ of rational numbers is approximation-like if for
    any integer $P$ of your choosing there is some $N$ such that all
    terms after $x_N$ will differ by less than $1/P$, \ie for all
    $n,m > N$,
    \[
    |x_n-x_m| < \frac 1 P
    \]
  \end{sloppypar}
\end{defn}
\begin{figure}[h]
  \centering
  \caption{First six terms of an \nameref{def:cauchy} sequence. Once the sequence enters a circle, it never leaves. The circles give an upper bound on the error in the approximation, analogous to the $1/P$ terms.}
  \begin{asy}
    import graph;
    size(200);
    fill(Circle(1.6,.38),lightblue);
    fill(Circle(1.45,.2),lightred);
    fill(Circle(1.4,.12),lightgreen);
    fill(Circle(1.43,.08),lightblue);
    dot((1.1,0));
    label("$x_0$",(1.1,0),S);
    dot((1.8,0));
    label("$x_1$",(1.8,0),S);
    dot((1.6,0));
    label("$x_2$",(1.6,0),S);
    dot((1.3,0));
    label("$x_3$",(1.3,0),S);
    dot((1.4,0));
    label("$x_4$",(1.4,0),S);
    dot((1.43,0));
    label("$x_5$",(1.43,0),N);
    dot((1.44,0));
    label("$x_6$",(1.44,0),SE);
    draw((1,0)--(1.9,0),gray(.3));
\end{asy}
\label{fig:cauchy}
\end{figure}
\begin{defn}[\sim]
  Two \nameref{def:cauchy} sequences are $\sim$-equivalent iff the difference in corresponding elements shrinks to $0$
  \[
  (x_0,x_1,\dots x_n\dots) \sim (y_0,y_1,\dots\ y_n,\dots) \iff \lim_{n\to\infty} |x_n-y_n| = 0
  \]
\end{defn}
\begin{defn}[$\R$]
  Let $A$ be the set of \nameref{def:cauchy} sequences of rationals, a subset of $\Q^\N$ we can define by specification (\zfc axiom \ref{zfc:spec}). Define the real numbers
  \[
  \R=A/\sim
  \]
  with addition given by
  \[
  (x_0,x_1,\dots) + (y_0,y_1,\dots) = (x_0+y_0,x_1+y_1,\dots)
  \]
  negation by
  \[
  -(x_0,x_1,\dots)=(-x_0,-x_1,\dots)
  \]
  and multiplication by
  \[
  (x_0,x_1,\dots)\cdot (y_0,y_1,\dots) = (x_0y_0,x_1y_1,\dots)
  \]
  For simplicity, we omit the checks that these operations output elements of $A$, as well as the check that equivalent inputs yield equivalent outputs. 
\end{defn}
\printbibliography
\end{document}
#+ORGTBL: SEND and-tt orgtbl-to-latex :splice nil :skip 0 
| $\wedge$ | 0 | 1 |
|        0 | 0 | 0 |
|        1 | 0 | 1 |
#+TBLFM: @2$2..@3$3=@1 && $1

#+ORGTBL: SEND or-tt orgtbl-to-latex :splice nil :skip 0 
| $\vee$ | 0 | 1 |
|      0 | 0 | 1 |
|      1 | 1 | 1 |
#+TBLFM: @2$2..@3$3=$1 || @1

#+ORGTBL: SEND not-tt orgtbl-to-latex :splice nil :skip 0 
| $x$ | $\neg x$ |
|   0 |        1 |
|   1 |        0 |

#+ORGTBL: SEND nand-dmorg orgtbl-to-latex :splice nil :skip 0 
| $x$ | $y$ | $x\wedge y$ | $\neg(x\wedge y)$ | $\neg x$ | $\neg y$ | $(\neg x)\vee (\neg y)$ |
|-----+-----+-------------+-------------------+----------+----------+-------------------------|
|   0 |   0 |           0 |                 1 |        1 |        1 |                       1 |
|   0 |   1 |           0 |                 1 |        1 |        0 |                       1 |
|   1 |   0 |           0 |                 1 |        0 |        1 |                       1 |
|   1 |   1 |           1 |                 0 |        0 |        0 |                       0 |
#+TBLFM: $3=$1 && $2::$4=! $3::$5=!$1::$6=!$2::$7=$5 || $6

#+ORGTBL: SEND if-tt orgtbl-to-latex :splice nil :skip 0 
| $\implies$ | 0 | 1 |
|------------+---+---|
|          0 | 1 | 1 |
|          1 | 0 | 1 |

#+ORGTBL: SEND bool-complete orgtbl-to-latex :splice nil :skip 0
| $x_1$ | $x_2$ | $x_3$ | f |
|-------+-------+-------+---|
|     0 |     0 |     0 | 0 |
|     0 |     0 |     1 | 1 |
|     0 |     1 |     0 | 0 |
|     0 |     1 |     1 | 0 |
|     1 |     0 |     0 | 1 |
|     1 |     0 |     1 | 0 |
|     1 |     1 |     0 | 1 |
|     1 |     1 |     1 | 0 |


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
